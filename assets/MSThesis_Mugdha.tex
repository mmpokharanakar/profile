\documentclass[12pt,a4paper,bold]{thesis}
%changed from report to thesis by Mugdha

% ------------------------------
% Information on how to use this file
% ------------------------------
%
% This file was prepared under the MikTex distribution <http://miktex.org/>
% All packages such as "thesis", "amsmath", "graphicx", "epstopdf", etc. are available on <http://ctan.org/>.
% Most Latex editors (such as WinEdt/TexMaker/Texniccenter) will download these for you as you try to compile this file.
% Please change the Individual Details below before you compile.
% The following pages are optional. You can comment out the corresponding lines before compiling :
% - List of Symbols
% - Appendix
%
% Note: Often, you will have to compile the file twice to see the changes correctly. This is because of the way the \tableofcontents command works.
%
% In case of any questions, please contact Dr. Prahlad Vaidyanathan <prahlad@iiserb.ac.in>

% ------------------------------
% Preamble
% ------------------------------

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{setspace}
\onehalfspacing
\allowdisplaybreaks %Mugdha
\usepackage{hyperref} %added by Mugdha

\usepackage{graphicx}
\usepackage{epstopdf}

%\usepackage[toc, page]{appendix}
\usepackage[titles]{tocloft}
\usepackage{xurl}
\usepackage[capitalise]{cleveref}
\usepackage[msc-links, alphabetic]{amsrefs}
%last 4 lines added by Mugdha

%\theoremstyle{thm} (Commented out by Mugdha)
\newtheorem{thm}{Theorem}[chapter]
\theoremstyle{definition}
\newtheorem{exmp}[thm]{Example}
\newtheorem{defn}[thm]{Definition}
\newtheorem{remark}[thm]{Remark}
\newtheorem{lemma}[thm]{Lemma}

%\providecommand{\appendixname}{Appendices}
%\newcommand{\appendixtocname}{Appendices}
%\newcommand{\appendixpagename}{Appendices}

\newcommand{\headcerti}[1]{
\vspace{3em}
\begin{center}
\LARGE{\MakeUppercase{\textbf{#1}}}
\end{center}
\vspace{3em}
}
%added by Jyoti sir, edited by Mugdha

\newcommand{\head}[1]{\newpage
\phantomsection %Mugdha
\addcontentsline{toc}{chapter}{#1}
\vspace{3em}
\begin{center}
\LARGE{\MakeUppercase{\textbf{#1}}}
\end{center}
\vspace{3em}
}
%edited by Mugdha

\renewcommand{\listfigurename}{} 

% \usepackage{amsthm}
% \usepackage{mathrsfs}
% \usepackage{amssymb}
% \usepackage{setspace} Commented out by Mugdha
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows} %Mugdha
\usetikzlibrary{plotmarks}
\usepackage[super]{nth}
\onehalfspacing
\usepackage{fancyhdr}
\usepackage{enumitem}
% \usepackage{graphicx}
\usepackage{wallpaper}
\usepackage{epigraph}
% \usepackage{epstopdf}
\usepackage[utf8]{inputenc}
\usepackage{mathtools, amsmath}

\renewcommand{\cftchapleader}{\cftdotfill{\cftdotsep}}
%added by Mugdha

%function name with domain and codomain
\newcommand*{\map}[3]{#1 \colon #2 \to #3} 

%absolute value
\newcommand*{\abs}[1]{\left\vert #1 \right\vert} 

%inner product
\newcommand*{\ip}[2]{\left\langle #1 , #2 \right\rangle} 

%norm: If you want a subscript to the norm, then write it as 
%the first argument in [] (e.g. \norm[2]{f} to print l^2 norm of f)
\newcommand*{\norm}[2][]{\left\Vert #2 \right\Vert_{#1}} 

\newcommand{\vol}{\ensuremath{\mathrm{vol}}}
\newcommand{\blue}[1]{{\color{blue} #1}}
\newcommand{\red}[1]{{\color{red} #1}}

%environment for cases
\newcounter{casenum}
\newenvironment{caseof}{\setcounter{casenum}{1}}{\vskip0\baselineskip}
\newcommand{\case}[2]{\vskip0\baselineskip\par\noindent 
{\bfseries Case \arabic{casenum}:} #1\\#2\addtocounter{casenum}{1}}

% ------------------------------
% Individual details (Change these!)
% ------------------------------

\newcommand{\thesistitle}{Expansion in Graphs and Graph Limits}
\newcommand{\studentname}{Pokharanakar Mugdha Mahesh}
\newcommand{\studentrollno}{220403}
\newcommand{\advisorname}{Dr. Jyoti Prakash Saha}
\newcommand{\degreename}{MS}
\newcommand{\subject}{Mathematics}
\newcommand{\thesisdate}{April 2025}

% ------------------------------
% Title page
% ------------------------------

\def\maketitle{
\begin{titlepage}
\begin{center}
\begin{doublespace}
\textbf{\MakeUppercase{\LARGE{\thesistitle}}} \\
\ \\
%removed \ \\ by Mugdha
\normalsize{\textbf{A THESIS}} \\
\normalsize{\textit{submitted in partial fulfillment of the requirements}} \\
\normalsize{\textit{for the award of the degree of}} \\
\ \\
%removed \ \\ by Mugdha
\large{\textbf{Master of Science}} \\
%changed Bac...-Master... to Master of Science by Mugdha
\normalsize{\textit{in}} \\
\large{\textbf{\MakeUppercase{\subject}}} \\
\normalsize{\textit{by}} \\
\large{\textbf{\MakeUppercase{\studentname}}} \\
\normalsize{\textbf{(\studentrollno)}} \\
%\normalsize{\textit{Under the guidance of}} \\
%\large{\textbf{\MakeUppercase{\advisorname}}}
\end{doublespace}
\vfill
\centerline{\includegraphics[scale=0.20]{iiser_b.png}}
%restored the above Ln by Mugdha
\ \\ 
\textbf{DEPARTMENT OF \MakeUppercase{\subject} \\ 
INDIAN INSTITUTE OF SCIENCE EDUCATION AND RESEARCH BHOPAL\\ %Flows onto two lines
BHOPAL - 462066} \\ 
\ \\
\textbf{\thesisdate}
\end{center}
\end{titlepage}
}

\setlength{\cftsecnumwidth}{2.15em}
\setlength{\cftsubsecnumwidth}{2.5em} 
%Jyoti sir
% ------------------------------
\begin{document}
\maketitle

\pagenumbering{roman}

% ------------------------------
\phantomsection %Mugdha
\addcontentsline{toc}{chapter}{Certificate} %Mugdha
\fancyhf{}
\centerline{\includegraphics[scale=0.6]{cert1.png}}
%\ThisULCornerWallPaper{.95}{IISERLetterhead.pdf}
\setlength{\wpYoffset}{1.90cm}
\quad \vspace{-1cm}
\headcerti{Certificate}

\thispagestyle{empty} %added by Mugdha

This is to certify that {\bf \studentname}, Integrated Ph.D. student in the Department of
\subject, has completed bonafide work on the thesis entitled {\bf `\thesistitle'} under my
supervision and guidance.
%changed BS-MS(dual degree) to Integrated Ph.D. by Mugdha

\vspace{5em} %shifted below by one Ln by Mugdha
%\noindent
%hspace{1cm}

\textbf{\thesisdate \hfill \advisorname \\IISER Bhopal}

\vspace{2cm}
\begin{center}
\begin{tabular}{ccc}
\textbf{Committee Member} & \textbf{Signature} & \textbf{Date} \\
\\
\rule{14em}{0.4pt} & \rule{10em}{0.4pt} & \rule{6em}{0.4pt} \\
\\
\rule{14em}{0.4pt} & \rule{10em}{0.4pt} & \rule{6em}{0.4pt} \\
\\
\rule{14em}{0.4pt} & \rule{10em}{0.4pt} & \rule{6em}{0.4pt} \\
\end{tabular}
\end{center}
%increased the lengths of the first two line segments by Mugdha

% ------------------------------
\head{Academic Integrity and Copyright Disclaimer}

I hereby declare that this project is my own work and, to the best of my knowledge, it contains no materials previously published or written by another
person, or substantial proportions of material which have been accepted for the award of any other degree or diploma at IISER Bhopal or any other educational
institution, except where due acknowledgement is made in the document. 

I certify that all copyrighted material incorporated into this document is in compliance with the Indian Copyright (Amendment) Act (2012) and that I have received written permission from the copyright owners for my use of their work, which is beyond the scope of the law. I agree to indemnify and save harmless IISER Bhopal from any and all claims that may be asserted or that may arise from any copyright violation.

%removed extra blank lines by Mugdha
%\vfill (Commented out by Mugdha)
\vspace{7em} %added by Mugdha
\textbf{\thesisdate \hfill \studentname \\ IISER Bhopal}

% ------------------------------
\head{Acknowledgement}

I would like to thank all my teachers since my school days, especially, Dr. Swapnaja Mohite,
who introduced me to the world of science and scientific research, and Dr. Rajeev Sapre,
with whom I used to discuss mathematics during my highschool and undergraduate days, 
and who helped me to find my passion for mathematics. Also, I got to learn a lot 
from teachers and peers in the MTTS programs. I am thankful to them.

At IISER Bhopal, I could study various courses under the guidance of wonderful teachers.
I am grateful to Dr. Atreyee Bhattacharya, Dr. Dheeraj Kulkarni, Dr. Prahlad Vaidyanathan, 
Dr. Rahul Garg, and Dr. Rohit Holkar for their valuable advices and moral support,
especially during my first year at IISER Bhopal. Dr. Jyoti Prakash Saha introduced me 
to representation theory of finite groups and additive combinatorics in the summer 
of 2023. I did a reading course in graph theory under his guidance during 
the second semester of the academic year 2023--2024. Dr. Kartick Adhikari,
Dr. Shashank Singh, and Asrafunnesa also attended my presentations. I thank them
for their useful comments during the presentations. This course helped me in 
developing my interest in graph theory, and preparing myself for the MS thesis.

It is a pleasure to express my gratitude to my thesis supervisor Dr. Jyoti Prakash Saha,
for his guidance, immense support and encouragement. I am grateful to him for
having a number of extensive and very useful discussions, and listening even to 
my naive ideas patiently. Also, I thank the other thesis evaluation committe members 
Dr. Kartick Adhikari and Dr. Rahul Garg for their support. 

I also appreciate the staff of the department of Mathematics for their help. 
I acknowledge the fellowship for the integrated Ph.D. program from IISER Bhopal.

I am thankful to my friends Aditi, Asrafunnesa, Chhavi, Dev, Muktai, Nishtha, 
Omkar, Shraddha, Swastika, and others for helping and supporting me whenever needed.

I am lucky to have family members who cultivated the habit of self-study in me,
and who always inspire me to keep working hard to acquire knowledge, through 
their action. I am deeply indebted to them for being with me in every situation, 
helping me in all possible ways, and encouraging me to reach new heights.

\vspace{7em}

\begin{flushright}
    {\bf \studentname}
\end{flushright}

% ------------------------------
\head{Abstract}

% A good abstract is concise, readable, and quantitative. The length should be approximately one paragraph, two at the most, or approximately from 200 to 400 words. Explain in one line why the project is important and summarize the major results. The final sentences explain the major implications of your work. Modern scientific style prefers the active voice. Abstracts are often an exception, but only if the passive voice reduces the total number of letters and words. 

% Do not repeat information that is in the title. Be explicit. Use numbers and quantifiable information where appropriate. Compose the abstract after you have read your report for the last time. Consider answering these questions to direct the content of the abstract: 1. What did you do? 2. Why did you do it? Which basic question were you trying to answer? 3. How did you do it? State methods. 4. What did you learn? State major results. 5. Why does it matter, what is the
% significance of your work? Identify one significant implication.

The discrete Cheeger--Buser inequality was established for graphs by Dodziuk, 
Alon and Milman. It states that the combinatorial expansion in graphs is equivalent to
the (one-sided) spectral expansion, that is, the Cheeger constant of a graph and
the gap between the two smallest eigenvalues of the Laplacian of the graph 
control each other. The analogous inequality, called the dual Cheeger--Buser
inequality, relating the bipartiteness ratio (or the dual Cheeger constant) 
of a graph and the gap between $2$ and the largest eigenvalue of the Laplacian 
of the graph was obtained by Trevisan and Bauer--Jost. We present its proof, 
following Trevisan's ideas, in the first chapter of the thesis. In the second chapter,
we discuss the higher-order Cheeger inequalities for regular graphs, based on 
the work of Lee, Oveis Gharan and Trevisan. These inequalities give the relation 
between the $k$-way expansion constant of a graph and the $k$th smallest eigenvalue 
of its Laplacian. The case $k = 2$ corresponds to the discrete Cheeger--Buser inequality.

The theory of graph limits was developed by Lov\'asz and his collaborators.
Recently, Khetan and Mj introduced the notion of Cheeger constant and the Laplacian
for graph limits, namely graphons and graphings, and extended the discrete 
Cheeger--Buser inequality for graphs to these graph limits. We define the bipartiteness 
ratio of graphons, and establish the dual Cheeger--Buser inequality for graphons, 
in the last chapter. We also show that the discrete Cheeger--Buser inequality 
for graphs can be recovered from it, upto a multiplicative constant.

%removed extra blank lines by Mugdha
% ------------------------------
% \head{List of Symbols or Abbreviations}

% \begin{center}
% \begin{tabular}{l@{\hspace{7em}}l@{}} \smallskip
% 	% $\alpha$ & The first letter \\ \smallskip
% 	% $\omega$ & The last letter \\ \smallskip
% 	% $\zeta(s)$ & The Riemann Zeta function \\ \smallskip
% \end{tabular}
% \end{center}

% % ------------------------------
% \head{List of Figures}

% \begin{center}
% \begin{tabular}{l@{\hspace{7em}}l@{}} \smallskip
% 	Proof sketch of \cref{thm:cheeger-ineq} & \cref{fig:HOCIProofSketch} 
% 	(\red{replace with p.no})
% 	\\ \smallskip
% \end{tabular}
% \end{center}

% ------------------------------
% \head{List of Tables}
% \begin{center}
% \begin{tabular}{l@{\hspace{7em}}r@{}} \smallskip
% 	Nonlinear Model Results & 5 \\ \smallskip
% \end{tabular}
% \end{center}
\tableofcontents

% ------------------------------
% \chapter{Introduction} \label{ch: introduction}
\pagenumbering{arabic}

\chapter{The dual Cheeger--Buser inequality for graphs} \label{ch:dualCBGraphs}

A discrete version of the Cheeger--Buser inequality on Riemmanian manifolds 
was established for simple graphs by Dodziuk \cite{Dodziuk84}, 
Alon--Milman \cite{Alon-Milman85}, and Alon \cite{Alon-vertexCheeger86}. 
It gives the following relation between the second smallest eigenvalue $\lambda$ 
of the Laplacian of a (finite undirected) graph and its Cheeger constant $h$.
\begin{equation*}
    \frac{h^2}{2} \leq \lambda \leq \sqrt{2h}.
\end{equation*}
The Cheeger constant of a graph quantifies how ``well-connected'' the graph is.
It is well-known that $\lambda$ is zero if and only if the graph is not connected. 
The discrete Cheeger--Buser inequality can be seen as a quantitative version of this fact.

On the other hand, it is known that the largest eigenvalue of the Laplacian 
of a graph is $2$ if and only if the graph has a bipartite connected component.
Trevisan \cite{Trevisan-MaxCut12} and Bauer--Jost \cite{Bauer-Jost13} defined 
the constants which measure ``bipartiteness'' of a graph, namely, the bipartiteness ratio 
$\beta(G)$ and the dual Cheeger constant $\bar{h}$ of a graph $G$, respectively, 
and they related these constants to the gap $2 - \lambda^{\max}$ between $2$ and 
the largest eigenvalue $\lambda^{\max}$ of the Laplacian of a graph $G$. This relation 
is known as the dual Cheeger--Buser inequality. We remark that Bauer and Jost used 
a technique developed by Desai and Rao \cite{Desai-Rao94} in their work.

Here, we present Trevisan's proof \cite{Trevisan-MaxCut12}
of the dual Cheeger--Buser inequality for weighted graphs. We have also referred 
to \cite{Trevisan-notes-expanders}*{Chapter 6} for some details in the proof.

\section{Preliminaries}

Let $V$ be a finite set with $|V| = n \geq 2$. Let $\ell^2(V)$ denote 
the inner product space of functions $\map{f}{V}{\mathbb{R}}$ with the inner product
\begin{equation*}
    \ip{f}{g} \coloneq \sum_{v \in V} f(v)g(v),
\end{equation*}
and the norm 
\begin{equation*}
    \norm{f} \coloneq \sqrt{\ip{f}{f}},
\end{equation*}
for all $f, g \in \ell^2(V)$. For any subset $A$ of $V$, we denote the characteristic
function of $A$ by $1_A$, and if $A$ is the singleton set $\{a\}$, then we use 
the notation $1_a$ to denote $1_A$. Let $\map{T}{\ell^2(V)}{\ell^2(V)}$ be 
a self-adjoint operator with $a_{uv} \coloneq \ip{T1_v}{1_u} \geq 0$ and  
$d_v \coloneq \ip{T1_V}{1_v} > 0$ for all $u,v \in V$. 
Further, let $\map{D}{\ell^2(V)}{\ell^2(V)}$ denote the linear operator defined by 
$(Df)(v) \coloneq d_v f(v)$, for all $f \in \ell^2(V)$, 
and $L$ denote the linear operator $I - D^{-1/2}TD^{-1/2}$ on $\ell^2(V)$, 
where $I$ denotes the identity operator on $\ell^2(V)$. 
Note that $L$ is a self-adjoint operator, and hence, it has $n$ real eigenvalues. 
We denote by $\lambda^{\max}$, the largest eigenvalue of $L$. Then, we have
\begin{equation*}
    \lambda^{\max} = \max_{f \in \ell^2(V) \setminus \{0\}} \frac{\ip{Lf}{f}}{\ip{f}{f}},
\end{equation*} 
and thus,
\begin{align} \label{eq:implicit-gap}
    2 - \lambda^{\max} 
    & = 
    \min_{f \in \ell^2(V) \setminus \{0\}} \frac{\ip{(2I - L)f}{f}}{\ip{f}{f}} \nonumber
    \\
    & = 
    \min_{f \in \ell^2(V) \setminus \{0\}} 
    \frac{\ip{(I + D^{-1/2}TD^{-1/2})f}{f}}{\ip{f}{f}} \nonumber
    \\
    & =
    \min_{f \in \ell^2(V) \setminus \{0\}} 
    \frac{\ip{(I + D^{-1/2}TD^{-1/2})(D^{1/2}f)}{D^{1/2}f}}{\ip{D^{1/2}f}{D^{1/2}f}} \nonumber
    \\
    & =
    \min_{f \in \ell^2(V) \setminus \{0\}} 
    \frac{\ip{D^{1/2}(I + D^{-1/2}TD^{-1/2})D^{1/2}f}{f}}{\ip{Df}{f}} \nonumber
    \\
    & =
    \min_{f \in \ell^2(V) \setminus \{0\}} \frac{\ip{(D + T)f}{f}}{\ip{Df}{f}}, 
\end{align} 
that is,
\begin{align} \label{eq:explicit-gap}
    2 - \lambda^{\max}
    & =
    \min_{f \in \ell^2(V) \setminus \{0\}} \frac{\sum_{v \in V} d_v f(v)^2 
    + \sum_{u,v \in V} a_{uv} f(u) f(v)}{\sum_{v \in V} d_v f(v)^2} \nonumber
    \\
    & =
    \min_{f \in \ell^2(V) \setminus \{0\}} 
    \frac{\sum_{u,v \in V} a_{uv} (f(u) + f(v))^2}{2 \sum_{v \in V} d_v f(v)^2}.
\end{align}

\begin{defn}[Bipartiteness ratio]
    Given any linear operator $\map{T}{\ell^2(V)}{\ell^2(V)}$ as above, 
    the \emph{bipartiteness ratio} $\beta_T$ of $T$ is defined by
    \begin{equation*}
        \beta_T = \min_{\substack{\map{\psi}{V}{\{-1,0,1\}} \\ \psi \neq 0}} 
        \frac{\ip{(D+T)\psi}{\psi}}{2 \ip{D \psi}{\psi}}.
    \end{equation*}
\end{defn}

Note that
\begin{align} 
    \beta_T 
    & = 
    \min_{\substack{\map{\psi}{V}{\{-1,0,1\}} \\ \psi \neq 0}} 
    \frac{\sum_{u,v \in V} a_{uv} (\psi(v)^2 + \psi(u)\psi(v))}
    {2 \sum_{v \in V} d_v \psi(v)^2} \label{eq:explicit-betaI}
    \\
    & =
    \min_{\substack{\map{\psi}{V}{\{-1,0,1\}} \\ \psi \neq 0}} 
    \frac{\sum_{u,v \in V} a_{uv} (\psi(u) + \psi(v))^2}
    {4 \sum_{v \in V} d_v \psi(v)^2}. \label{eq:explicit-betaII}
\end{align}

\section[The dual Cheeger--Buser inequality for graphs]
{The dual Cheeger--Buser inequality for graphs
\sectionmark{The dual Cheeger--Buser ineq. for graphs}}
\sectionmark{The dual Cheeger--Buser ineq. for graphs}

\begin{thm}[The dual Cheeger--Buser inequality for graphs] \label{thm:dualCBGraphs}
    For every linear operator $\map{T}{\ell^2(V)}{\ell^2(V)}$ as above, the inequality
    \begin{equation*}
        \frac{\beta_T^2}{2} \leq 2 - \lambda^{\max} \leq 2 \beta_T
    \end{equation*}
    holds.
\end{thm}

We will use the following lemma in the proof of the above theorem. 

\begin{lemma} \label{lemma:f/g&intf/intg}
    Let $(\Omega, \mathcal{A}, \mu)$ be a measure space with $\mu(\Omega) > 0$,
    and $\map{f}{\Omega}{\mathbb{R}}$, $\map{g}{\Omega}{(0,\infty)}$ be integrable
    functions. Then there exists an $x_0 \in \Omega$ such that 
    \begin{equation*}
        \frac{f(x_0)}{g(x_0)} \leq \frac{\int_{\Omega} f(x)\, \mathrm{d}\mu(x)}
        {\int_{\Omega} g(x)\, \mathrm{d}\mu(x)}.
    \end{equation*}
\end{lemma}

\begin{proof}
    Since the function $g$ takes only positive values and $\mu(\Omega) > 0$,
    we have $\int_{\Omega} g(x)\, \mathrm{d}\mu(x) > 0$. Further, note that 
    \begin{equation} \label{eq:int0}
        \int_{\Omega} \left(f(x) - \frac{\int_{\Omega} f(t)\, \mathrm{d}\mu(t)}
        {\int_{\Omega} g(t)\, \mathrm{d}\mu(t)} g(x) \right) \mathrm{d}\mu(x) = 0.
    \end{equation}
    If $f(x) - \frac{\int_{\Omega} f(t)\, \mathrm{d}\mu(x)}
    {\int_{\Omega} g(t)\, \mathrm{d}\mu(t)}g(x)$ is positive for all $x$, then 
    its integration over $\Omega$ is positive, as $\mu(\Omega)$ is positive,
    which contradicts \cref{eq:int0}. Hence, there is an $x_0 \in \Omega$ such that 
    \begin{equation*}
        f(x_0) - \frac{\int_{\Omega} f(x_)\, \mathrm{d}\mu(x)}
        {\int_{\Omega} g(x)\, \mathrm{d}\mu(x)}g(x_0) \leq 0,
    \end{equation*}
    that is
    \begin{equation*}
        \frac{f(x_0)}{g(x_0)} \leq \frac{\int_{\Omega} f(x)\, \mathrm{d}\mu(x)}
        {\int_{\Omega} g(x)\, \mathrm{d}\mu(x)}.
    \end{equation*}
\end{proof}

\begin{proof}[Proof of \cref{thm:dualCBGraphs}]
    The inequality $2 - \lambda^{\max} \leq 2 \beta_T$ follows immediately 
    from \cref{eq:implicit-gap} and the definition of $\beta_T$. To get the other inequality, 
    thanks to \cref{eq:explicit-gap} and \cref{eq:explicit-betaI}, it suffices
    to show that given any nonzero function $\map{f}{V}{\mathbb{R}}$, there is 
    a nonzero function $\map{\psi}{V}{\{-1,0,1\}}$ such that
    \begin{equation} \label{eq:maineq}
        \frac{\sum_{u,v \in V} a_{uv} (\psi(v)^2 + \psi(u)\psi(v))}{2 \sum_{v \in V} d_v \psi(v)^2}
        \leq \left(\frac{\sum_{u,v \in V} a_{uv} (f(u) + f(v))^2}
        {\sum_{v \in V} d_v f(v)^2}\right)^{\frac{1}{2}}.
    \end{equation}

    Let $\map{f}{V}{\mathbb{R}}$ be a nonzero function, and $M$ denote the positive number
    $\max_{v \in V} \abs{f(v)}$. For every $0 < t \leq M$, define the function 
    $\map{\psi_t}{V}{\{-1,0,1\}}$ by
    \begin{equation*}
        \psi_t(v) = 
        \begin{cases}
            -1 & \text{if } f(v) \leq -t,
            \\
            0 & \text{if } -t < f(v) < t,
            \\
            1 & \text{if } f(v) \geq t.
        \end{cases}
    \end{equation*}
    For all $0 < t \leq M$, note that the function $\psi_t$ is nonzero. We will show 
    using \cref{lemma:f/g&intf/intg} that \cref{eq:maineq} is satisfied by 
    $\psi = \psi_{t_0}$ for some $0 < t_0 \leq M$. 
    
    Observe that
    \begin{align*}
        & \hspace{0.49cm} 
        \int_{0}^{M} 2t \sum_{u,v \in V} a_{uv} (\psi_t(v)^2 + \psi_t(u)\psi_t(v))\, \mathrm{d}t 
        \\
        & = 
        \sum_{u,v \in V} a_{uv} \int_{0}^{M} 2t (\psi_t(v)^2 + \psi_t(u)\psi_t(v))\, \mathrm{d}t 
        \\
        & = 
        \sum_{\substack{u,v \in V \\ f(u)f(v) \geq 0 \\ \abs{f(u)} \leq \abs{f(v)}}} 
        a_{uv} \int_{0}^{M} 2t (\psi_t(v)^2 + \psi_t(u)\psi_t(v))\, \mathrm{d}t
        \\
        & \quad + 
        \sum_{\substack{u,v \in V \\ f(u)f(v) \geq 0 \\ \abs{f(v)} < \abs{f(v)}}} 
        a_{uv} \int_{0}^{M} 2t (\psi_t(v)^2 + \psi_t(u)\psi_t(v))\, \mathrm{d}t
        \\
        & \quad + 
        \sum_{\substack{u,v \in V \\ f(u)f(v) < 0 \\ \abs{f(u)} \leq \abs{f(v)}}} 
        a_{uv} \int_{0}^{M} 2t (\psi_t(v)^2 + \psi_t(u)\psi_t(v))\, \mathrm{d}t.
        \\
        & \quad + 
        \sum_{\substack{u,v \in V \\ f(u)f(v) < 0 \\ \abs{f(v)} < \abs{f(u)}}} 
        a_{uv} \int_{0}^{M} 2t (\psi_t(v)^2 + \psi_t(u)\psi_t(v))\, \mathrm{d}t
    \end{align*} 
    Let $u, v$ be arbitrary elements of $V$. Consider the following cases.
    \begin{caseof}
        \case{$f(u)f(v) \geq 0$ and $\abs{f(u)} \leq \abs{f(v)}$}
        {Suppose that $0 \leq f(u) \leq f(v)$. 
        \vspace{-0.4\baselineskip} 
        \begin{itemize}[leftmargin=*,noitemsep]
            \item If $0 < t \leq f(u)$, then $\psi_t(u) = \psi_t(v) = 1$.
            \item If $f(u) < t \leq f(v)$, then $\psi_t(u) = 0$ and $\psi_t(v) = 1$.
            \item If $t > f(v)$, then $\psi_t(u) = \psi_t(v) = 0$.
        \end{itemize}
        \vspace{-0.4\baselineskip}
        So, we have 
        \begin{align*}
            \int_{0}^{M} 2t (\psi_t(v)^2 + \psi_t(u)\psi_t(v))\, \mathrm{d}t 
            & = 
            \int_{0}^{f(u)} 4t \, \mathrm{d}t + \int_{f(u)}^{f(v)} 2t\, \mathrm{d}t
            \\
            & =
            2f(u)^2 + f(v)^2 - f(u)^2
            \\
            & =
            f(u)^2 + f(v)^2.
        \end{align*}
        Now, suppose that $f(v) \leq f(u) \leq 0$.
        \vspace{-0.4\baselineskip} 
        \begin{itemize}[leftmargin=*,noitemsep]
            \item If $0 < t \leq -f(u)$, then $\psi_t(u) = \psi_t(v) = -1$.
            \item If $-f(u) < t \leq -f(v)$, then $\psi_t(u) = 0$ and $\psi_t(v) = -1$.
            \item If $t > -f(v)$, then $\psi_t(u) = \psi_t(v) = 0$.
        \end{itemize}
        \vspace{-0.4\baselineskip}
        Thus, here also, we get
        \begin{equation*}
            \int_{0}^{M} 2t (\psi_t(v)^2 + \psi_t(u)\psi_t(v))\, \mathrm{d}t = f(u)^2 + f(v)^2.
        \end{equation*}}

        \case{$f(u)f(v) \geq 0$ and $\abs{f(v)} < \abs{f(u)}$}
        {Using the arguments similar to those in the above case, we obtain
        \begin{equation*}
            \int_{0}^{M} 2t (\psi_t(v)^2 + \psi_t(u)\psi_t(v))\, \mathrm{d}t 
            = \int_{0}^{\abs{f(v)}} 4t\, \mathrm{d}t = 2f(v)^2.
        \end{equation*}}

        \case{$f(u)f(v) < 0$ and $\abs{f(u)} \leq \abs{f(v)}$}
        {Suppose that $f(u) < 0 < f(v)$. 
        \vspace{-0.4\baselineskip} 
        \begin{itemize}[leftmargin=*,noitemsep]
            \item If $0 < t \leq -f(u)$, then $\psi_t(u) = -1$ and $\psi_t(v) = 1$.
            \item If $-f(u) < t \leq f(v)$, then $\psi_t(u) = 0$ and $\psi_t(v) = 1$.
            \item If $t > f(v)$, then $\psi_t(u) = \psi_t(v) = 0$.
        \end{itemize}
        \vspace{-0.4\baselineskip}
        So, we have 
        \begin{equation*}
            \int_{0}^{M} 2t (\psi_t(v)^2 + \psi_t(u)\psi_t(v))\, \mathrm{d}t 
            = \int_{-f(u)}^{f(v)} 2t\, \mathrm{d}t
            = f(v)^2 - f(u)^2.
        \end{equation*}
        Now, suppose that $f(v) < 0 < f(u)$.
        \vspace{-0.4\baselineskip} 
        \begin{itemize}[leftmargin=*,noitemsep]
            \item If $0 < t \leq f(u)$, then $\psi_t(u) = 1$ and $\psi_t(v) = -1$.
            \item If $f(u) < t \leq -f(v)$, then $\psi_t(u) = 0$ and $\psi_t(v) = -1$.
            \item If $t > -f(v)$, then $\psi_t(u) = \psi_t(v) = 0$.
        \end{itemize}
        \vspace{-0.4\baselineskip}
        Thus, here also, we get
        \begin{equation*}
            \int_{0}^{M} 2t (\psi_t(v)^2 + \psi_t(u)\psi_t(v))\, \mathrm{d}t = f(v)^2 - f(u)^2.
        \end{equation*}}

        \case{$f(u)f(v) < 0$ and $\abs{f(v)} < \abs{f(u)}$}
        {Using the arguments similar to those in the above case, note that
        $\psi_t(v)^2 + \psi_t(u)\psi_t(v) = 0$, for all $0 < t \leq M$.}
    \end{caseof}
    Hence, we have
    \begin{align} \label{ineq:num-betaT}
        & \hspace{0.49cm}
        \int_{0}^{M} 2t \sum_{u,v \in V} a_{uv} 
        (\psi_t(v)^2 + \psi_t(u)\psi_t(v))\, \mathrm{d}t \nonumber
        \\ 
        & = 
        \sum_{\substack{u,v \in V \\ f(u)f(v) \geq 0 \\ \abs{f(u)} \leq \abs{f(v)}}} 
        a_{uv} (f(u)^2 + f(v)^2)
        + \sum_{\substack{u,v \in V \\ f(u)f(v) \geq 0 \\ \abs{f(v)} < \abs{f(u)}}} 
        2 a_{uv} f(v)^2 \nonumber
        \\
        & \quad +
        \sum_{\substack{u,v \in V \\ f(u)f(v) < 0 \\ \abs{f(u)} \leq \abs{f(v)}}} 
        a_{uv} (f(v)^2 - f(u)^2) \nonumber
        \\
        & \leq
        \sum_{\substack{u,v \in V \\ f(u)f(v) \geq 0}} a_{uv} (f(u)^2 + f(v)^2)
        + \sum_{\substack{u,v \in V \\ f(u)f(v) < 0}} a_{uv} \abs{f(u)^2 - f(v)^2} \nonumber
        \\
        & \leq
        \sum_{\substack{u,v \in V \\ f(u)f(v) \geq 0}} a_{uv} (f(u) + f(v))^2
        + \sum_{\substack{u,v \in V \\ f(u)f(v) < 0}} a_{uv} \abs{f(u)^2 - f(v)^2} \nonumber
        \\
        & \leq
        \sum_{\substack{u,v \in V \\ f(u)f(v) \geq 0}} 
        a_{uv} (f(u) + f(v))(\abs{f(u)} + \abs{f(v)}) \nonumber
        \\
        & \quad +
        \sum_{\substack{u,v \in V \\ f(u)f(v) < 0}} a_{uv} \abs{f(u) + f(v)} \abs{f(u) - f(v)} \nonumber
        \\
        & \leq
        \sum_{u,v \in V} a_{uv} \abs{f(u) + f(v)}(\abs{f(u)} + \abs{f(v)}) \nonumber
        \\
        & \leq
        \left(\sum_{u,v \in V} a_{uv} (f(u) + f(v))^2\right)^\frac{1}{2}
        \left(\sum_{u,v \in V} a_{uv} (\abs{f(u)} + \abs{f(v)})^2\right)^\frac{1}{2} \nonumber
        \\
        & \leq
        \left(\sum_{u,v \in V} a_{uv} (f(u) + f(v))^2\right)^\frac{1}{2}
        \left(\sum_{u,v \in V} a_{uv} (2f(u)^2 + 2f(v)^2)\right)^\frac{1}{2} \nonumber
        \\
        & \leq
        \left(\sum_{u,v \in V} a_{uv} (f(u) + f(v))^2\right)^\frac{1}{2}
        \left(4 \sum_{u,v \in V} a_{uv} f(v)^2\right)^\frac{1}{2} \nonumber
        \\
        & =
        2 \left(\sum_{u,v \in V} a_{uv} (f(u) + f(v))^2\right)^\frac{1}{2}
        \left(\sum_{v \in V} d_v f(v)^2\right)^\frac{1}{2}. 
    \end{align}
    Also, note that
    \begin{align} \label{eq:den-betaT}
        \int_{0}^{M} 2t \cdot 2 \sum_{v \in V} d_v \psi_t(v)^2\, \mathrm{d}t
        & =
        \sum_{v \in V} d_v \int_{0}^{M} 4t \psi_t(v)^2\, \mathrm{d}t \nonumber
        \\
        & =
        \sum_{v \in V} d_v \int_{0}^{\abs{f(v)}} 4t\, \mathrm{d}t \nonumber
        \\
        & =
        2 \sum_{v \in V} d_v f(v)^2.
    \end{align}
    Now using the facts that $d_v$ is positive for all $v \in V$ and the function $\psi_t$
    is nonzero for all $t \in (0,M]$, and combining \eqref{ineq:num-betaT} with
    \eqref{eq:den-betaT} yield the inequality
    \begin{align*}
        & \hspace{0.49cm}
        \frac{\int_{0}^{M} 2t \sum_{u,v \in V} a_{uv} (\psi_t(v)^2 + \psi_t(u)\psi_t(v))\, 
        \mathrm{d}t}{\int_{0}^{M} 2t \cdot 2 \sum_{v \in V} d_v \psi_t(v)^2\, \mathrm{d}t}
        \\
        & \leq 
        \frac{2 \left(\sum_{u,v \in V} a_{uv} (f(u) + f(v))^2\right)^\frac{1}{2}
        \left(\sum_{v \in V} d_v f(v)^2\right)^\frac{1}{2}}{2 \sum_{v \in V} d_v f(v)^2}
        \\
        & =
        \left(\frac{\sum_{u,v \in V} a_{uv} (f(u) + f(v))^2}
        {\sum_{v \in V} d_v f(v)^2}\right)^\frac{1}{2}.
    \end{align*}
    Therefore, there exists a real number $t_0 \in (0, M]$ such that 
    \begin{equation*}
        \frac{\sum_{u,v \in V} a_{uv} (\psi_{t_0}(v)^2 + \psi_{t_0}(u)\psi_{t_0}(v))}
        {2 \sum_{v \in V} d_v \psi_{t_0}(v)^2} 
        \leq \left(\frac{\sum_{u,v \in V} a_{uv} (f(u) + f(v))^2}
        {\sum_{v \in V} d_v f(v)^2}\right)^\frac{1}{2},
    \end{equation*}
    using \cref{lemma:f/g&intf/intg}, as desired.
\end{proof}

% -----------------------------
\chapter{Higher-order Cheeger inequalities} \label{ch:HOCI}

\tikzstyle{lemmabox} = [rectangle, minimum width = 2cm, minimum height = 1cm,
text centered, draw = black]
\tikzstyle{arrow} = [thick, ->, >=stealth]

We have discussed in \cref{ch:dualCBGraphs} that the discrete Cheeger--Buser inequality 
gives a relation between the second smallest eigenvalue of the Laplacian of a graph 
and its Cheeger constant (also known as the edge expansion).
Lee, Oveis Gharan and Trevisan \cite{Higher-Cheeger} established 
an analog of this for other eigenvalues of the Laplacian of a graph.
As an analog of the Cheeger constant of a graph $G$ with vertex set $V$, they introduced 
the $k$-way expansion constant, for every $1 \leq k \leq |V|$. They proved that
for every graph $G$ with vertex set $V$ and every $1 \leq k \leq |V|$, the inequality
\begin{equation*}
	\frac{\lambda_k}{2} \leq \phi_G(k) \leq O(k^2) \sqrt{\lambda_k}
\end{equation*}
holds, where $\lambda_k$ is the $k$th smallest eigenvalue of the Laplacian
of $G$ and $\phi_G(k)$ is its $k$-way expansion constant. This is a quantitative
version of the fact that the graph $G$ has at least $k$ connected components 
if and only if the $k$th smallest eigenvalue of its Laplacian is $0$. 

The above bounds have been improved, and several generalizations of the above inequality
have been established. Also, various notions of expansions and higher-order Cheeger 
inequalities for them have been studied. For instance, see 
\cites{Higher-Cheeger,Liu-MultiwayDualCheeger15,Atay-Liu-CheegerSigned20,HOCIBuffers24}.

Here we will prove a weaker bound than the above. In the following, thinking of $V$ 
as the vertex set of a weighted regular graph, and $T$ as its adjacency matrix 
gives the higher-order Cheeger inequalities for weighted regular graphs. 
The proof follows the ideas in Trevisan's proof \cite{Trevisan-notes-expanders}.

\section{Preliminaries}

Let $V$ be a finite set with $|V| = n \geq 2$. Let $\ell^2(V)$ denote 
the inner product space of functions $\map{f}{V}{\mathbb{R}}$ with the inner product
\begin{equation*}
    \ip{f}{g} \coloneq \sum_{v \in V} f(v)g(v),
\end{equation*}
and the norm 
\begin{equation*}
    \norm{f} \coloneq \sqrt{\ip{f}{f}},
\end{equation*}
for all $f, g \in \ell^2(V)$. Let $\map{T}{\ell^2(V)}{\ell^2(V)}$ be a self-adjoint
operator with $a_{uv} \coloneq \ip{T1_v}{1_u} \geq 0$ and $\ip{T1_V}{1_u} = d > 0$,
for all $u, v \in V$. Let $\map{L}{\ell^2(V)}{\ell^2(V)}$ denote the positive-semidefinite 
operator $I - \frac{1}{d}T$, and 
\begin{equation*}
    \lambda_1 \leq \lambda_2 \leq \dots \leq \lambda_n 
\end{equation*}
denote the eigenvalues of $L$. Using the Courant--Fischer min-max theorem, 
for any $1 \leq k \leq n$, we have
\begin{equation} \label{eq:char-of-evalues}
    \lambda_k = \min_{\substack{W \leq \ell^2(V) \\ k \text{-dimensional}}}
    \max_{f \in W \setminus \{0\}} \frac{\ip{Lf}{f}}{\ip{f}{f}}.
\end{equation}
The quantity $\frac{\ip{Lf}{f}}{\ip{f}{f}}$ is called the \emph{Rayleigh quotient}
of $f$ with respect to $L$. We denote it by $R_L(f)$.

For any $f \in \ell^2(V)$, observe that
\begin{align*}
    \ip{Lf}{f} = \sum_{u \in V} (Lf)(u)f(u) 
    & = 
    \sum_{u \in V} \left(\left(I - \frac{1}{d}T\right)f\right)(u)f(u)
    \\
    & =
    \sum_{u \in V} \left[f(u)^2 - \frac{1}{d} \sum_{v \in V} a_{uv}f(v)f(u)\right]
    \\
    & = 
    \frac{1}{d} \sum_{u \in V} \left[d f(u)^2 - \sum_{v \in V} a_{uv}f(v)f(u)\right]
    \\
    & =
    \frac{1}{d} \sum_{u \in V} \left[\sum_{v \in V} a_{uv}f(u)^2 
    - \sum_{v \in V} a_{uv}f(v)f(u)\right]
	\\
	& = 
	\frac{1}{d} \sum_{u,v \in V} (a_{uv}f(u)^2 - a_{uv}f(v)f(u)), 
\end{align*}
and hence, we have
\begin{align}
    \ip{Lf}{f} & = \frac{1}{d} \sum_{u,v \in V} (a_{uv}f(u)^2 - a_{uv}f(v)f(u)) \nonumber
    \\
    & =
    \frac{1}{2d} \sum_{u,v \in V} (a_{uv}f(u)^2 - 2 a_{uv}f(v)f(u) + a_{uv}f(v)^2) \nonumber
    \\
    & = 
    \frac{1}{2d} \sum_{u,v \in V} a_{uv} (f(u) - f(v))^2. \label{eq:Lf.f}
\end{align}

\begin{defn}[$k$-way expansion constant]
	For every $1 \leq k \leq n$ and a linear operator $\map{T}{\ell^2(V)}{\ell^2(V)}$ 
    as above, the \emph{k-way expansion constant} of $T$, denoted by $\phi_T(k)$, 
    is defined by
	\begin{equation*}
    	\phi_T(k) \coloneq \min_{\substack{\emptyset \neq S_1, \dots, S_k \subseteq V \\ 
    	S_1, \dots, S_k \text{ disjoint}}} \max_{1 \leq i \leq k} \phi(S_i),
	\end{equation*}
	where for any nonempty subset $S$ of $V$, we denote by $\phi(S)$, 
	the \emph{edge expansion} of $S$, which is defined as follows.
	\begin{equation*}
		\phi(S) \coloneq \frac{\ip{T1_{V \setminus S}}{1_S}}{d|S|}.
	\end{equation*} 
\end{defn}

Note that $\phi_T(1) = 0$, and $\phi_T(2)$ is the edge Cheeger constant of $T$.

\begin{thm}
    For any opertor $T$ as above and $1 \leq k \leq n$, the following inequality holds.
    \begin{equation*}
        \frac{\lambda_k}{2} \leq \phi_T(k) \leq O(k^{3.5}) \sqrt{\lambda_k}.
    \end{equation*}
\end{thm}

Note that the above inequality holds trivially for $k = 1$, 
and the case $k = 2$ is the well-known discrete Cheeger--Buser inequality.

\section{Proof of the easy direction}

\begin{lemma}
    For any $1 \leq k \leq n$, the inequality $\lambda_k \leq 2 \phi_T(k)$ holds.
\end{lemma}

\begin{proof}
    Let $S_1, \dots, S_k$ be nonempty disjoint subsets of $V$ such that $\phi_T(k) 
    = \max_{1 \leq i \leq k} \phi(S_i)$. Let $W = \text{span}\{1_{S_1}, \dots, 1_{S_k}\}$, 
    which is a $k$-dimensional subspace of $\ell^2(V)$. We will show that the Rayleigh
    quotient of every nonzero function in $W$ is at most $2 \phi_k(T)$, so that
    we are done using \cref{eq:char-of-evalues}. 
    
    For any $1 \leq i \leq k$, note that
    \begin{align*}
        R_L(1_{S_i}) = \frac{\ip{L1_{S_i}}{1_{S_i}}}{\ip{1_{S_i}}{1_{S_i}}} 
        & = 
        \frac{\ip{\left(I - \frac{1}{d}T \right)1_{S_i}}{1_{S_i}}}{\ip{1_{S_i}}{1_{S_i}}} 
        \\
        & = 
        \frac{\ip{(dI - T)1_{S_i}}{1_{S_i}}}{d|S_i|} 
        \\ 
        & = 
        \frac{d|S_i| - \ip{T1_{S_i}}{1_{S_i}}}{d|S_i|}
        \\
        & =
        \frac{\ip{T1_V}{1_{S_i}} - \ip{T1_{S_i}}{1_{S_i}}}{d|S_i|}
        \\
        & =
        \frac{\ip{T1_{V \setminus S_i}}{1_{S_i}}}{d|S_i|}
        \\
        & =
        \phi(S_i)
        \\
        & \leq 
        \phi_T(k).
    \end{align*}
    Then, the desired inequality follows from the following lemma using the facts
    that for distinct $i$ and $j$, if $f_i$ and $f_j$ lie in the span of $1_{S_i}$ 
    and $1_{S_j}$, respectively, then the functions $f_i$ and $f_j$ are disjointly supported,
    and that the Rayleigh quotients are invariant under scaling.
\end{proof}

\begin{lemma}
    Let $f_1, f_2, \dots, f_k \in \ell^2(V)$ be disjointly supported nonzero functions.
    Then, we have the inequality
    \begin{equation*}
        R_L \left(\sum_{i=1}^{k} f_i\right) \leq 2 \max_{1 \leq i \leq k} R_L(f_i).
    \end{equation*}
\end{lemma}

\begin{proof}
    From \cref{eq:Lf.f}, it follows that
    \begin{equation*}
        \ip{L\left(\sum_{i=1}^{k} f_i\right)}{\sum_{i=1}^{k} f_i} 
        = \frac{1}{2d} \sum_{u,v \in V} a_{uv} \left(\sum_{i=1}^{k}(f_i(u) - f_i(v))\right)^2.
    \end{equation*}
    
    Let $u, v \in V$ be arbitrary. Since the functions $f_1, \dots, f_k$ are 
    disjointly supported, each of $u$ and $v$ is in the support of at most
    one of these functions. That is, there exist indices $j_u, l_v \in \{1, \dots, k\}$
    such that for any $i \neq j_u$, we have $f_i(u) = 0$ and for any $i \neq l_v$,
    we have $f_i(v) = 0$. Hence, we get
    \begin{equation*}
        \left(\sum_{i=1}^{k}(f_i(u) - f_i(v))\right)^2 = (f_{j_u}(u) - f_{l_v}(v))^2.
    \end{equation*}
    Now, if $j_u = l_v$, then the above equation implies that
    \begin{equation*}
        \left(\sum_{i=1}^{k}(f_i(u) - f_i(v))\right)^2 = \sum_{i=1}^{k} (f_i(u) - f_i(v))^2
        \leq 2 \sum_{i=1}^{k} (f_i(u) - f_i(v))^2,
    \end{equation*}
    and if $j_u \neq l_v$, then
    \begin{align*}
        \left(\sum_{i=1}^{k}(f_i(u) - f_i(v))\right)^2 
        & \leq 
        2(f_{j_u}(u))^2 + 2(f_{l_v}(v))^2
        \\
        & = 
        2 [(f_{j_u}(u) - 0)^2 + (0 - f_{l_v}(v))^2]
        \\
        & =
        2 \sum_{i = 1}^{k} (f_i(u) - f_i(v))^2.
    \end{align*}
    Thus, once again using \cref{eq:Lf.f}, we obtain
    \begin{align*}
        \ip{L\left(\sum_{i=1}^{k} f_i\right)}{\sum_{i=1}^{k} f_i} 
        & \leq 
        2 \frac{1}{2d} \sum_{u,v \in V} a_{uv} \sum_{i=1}^{k} (f_i(u) - f_i(v))^2
        \\
        & = 
        2 \sum_{i=1}^{k} \frac{1}{2d} \sum_{u,v \in V} a_{uv} (f_i(u) - f_i(v))^2
        \\
        & =
        2 \sum_{i=1}^{k} \ip{Lf_i}{f_i}.
    \end{align*}
    
    Note that the functions $f_1, \dots, f_k$, being disjointly supported, 
    are mutually orthogonal. So, we have
    \begin{equation*}
        \ip{\sum_{i=1}^{k} f_i}{\sum_{i=1}^{k} f_i} = \sum_{i=1}^{k} \ip{f_i}{f_i},
    \end{equation*}
    implying that
    \begin{equation*}
        R_L \left(\sum_{i=1}^{k} f_i\right) = \frac{\ip{L\left(\sum_{i=1}^{k} f_i\right)}
        {\sum_{i=1}^{k} f_i}}{\ip{\sum_{i=1}^{k} f_i}{\sum_{i=1}^{k} f_i}}
        \leq 2 \frac{\sum_{i=1}^{k} \ip{Lf_i}{f_i}}{\sum_{i=1}^{k} \ip{f_i}{f_i}}
        \leq 2 \max_{1 \leq i \leq k} R_L(f_i),
    \end{equation*}
    where the last inequality follows from the definition of the Rayleigh quotients 
    along with the fact that if $a_1, \dots, a_k$ are nonnegative real numbers 
    and $b_1, \dots, b_k$ are positive real numbers, then
    \begin{equation*}
        \sum_{i=1}^{k} a_i = \sum_{i=1}^{k} b_i \frac{a_i}{b_i} 
        \leq \max_{1 \leq i \leq k} \frac{a_i}{b_i} \sum_{i=1}^{k} b_i,
    \end{equation*}
    so that
    \begin{equation*}
        \frac{\sum_{i=1}^{k} a_i}{\sum_{i=1}^{k} b_i} 
        \leq \max_{1 \leq i \leq k} \frac{a_i}{b_i}.
    \end{equation*}
\end{proof}

\section{Proof of the difficult direction}

\begin{thm} \label{thm:cheeger-ineq}
    For any opertor $T$ as above and $1 \leq k \leq n$, the following inequality holds.
    \begin{equation*}
        \phi_T(k) \leq O(k^{3.5}) \sqrt{\lambda_k}.
    \end{equation*}
\end{thm}

We will break the proof of \cref{thm:cheeger-ineq} into several lemmas. 
Let us start by introducing some useful notions.

By abuse of notation, we denote the Euclidean inner product and the norm induced 
from it on $\mathbb{R}^k$ also, by $\ip{\cdot}{\cdot}$ and $\norm{\cdot}$, respectively.

Given functions $f_1, f_2, \dots, f_k \in \ell^2(V)$, define the function 
$\map{F}{V}{\mathbb{R}^k}$ by 
\begin{equation*}
    F(v) \coloneq (f_1(v), f_2(v), \dots, f_k(v)),
\end{equation*}
for all $v \in V$, which induces the following pseudo-metric $dist$ on $V$. 
For any $u, v \in V$, define
\begin{equation*}
    dist(u,v) \coloneq
    \begin{cases}
        \norm{\frac{F(u)}{\norm{F(u)}} - \frac{F(v)}{\norm{F(v)}}}
        & \text{if } F(u), F(v) \neq 0,
        \\
        0
        & \text{if } F(u) = F(v) = 0,
        \\
        \infty
        & \text{otherwise}.
    \end{cases} 
\end{equation*}
It is straightforward to check that this is indeed a pseudo-metric on $V$. 
Given any element $v$ of $V$ and a subset $A$ of $V$, we define 
\begin{equation*}
    dist(v, A) \coloneq \min_{u \in A} dist(v,u).
\end{equation*}

Also, we extend the notion of Rayleigh quotients for the functions taking values 
in $\mathbb{R}^k$. Given a function $\map{\mathbf{f}}{V}{\mathbb{R}^k}$, define the 
\emph{Rayleigh quotioent} $R_L(\mathbf{f})$ of $\mathbf{f}$ with respect to $L$ by 
\begin{equation*}
    R_L(\mathbf{f}) \coloneq \frac{\sum_{u,v \in V} a_{uv} \norm{\mathbf{f}(u) 
    - \mathbf{f}(v)}^2}{2d \sum_{v \in V} \norm{\mathbf{f}(v)}^2}.
\end{equation*}

Now if $f_1, f_2, \dots, f_k$ are unit vectors in $\ell^2(V)$, then we get
\begin{align}
    R_L(F) 
    & = 
    \frac{\sum_{u,v \in V} a_{uv} \norm{F(u) - F(v)}^2}{2d \sum_{v \in V} \norm{F(v)}^2} \nonumber
    \\
    & =
    \frac{\sum_{u,v \in V} a_{uv} \norm{F(u) - F(v)}^2}
    {2d \sum_{v \in V} \sum_{1 \leq i \leq k} f_i(v)^2} \nonumber
    \\
    & =
    \frac{\sum_{u,v \in V} a_{uv} \norm{F(u) - F(v)}^2}
    {2d \sum_{1 \leq i \leq k} \sum_{v \in V} f_i(v)^2} \nonumber
    \\
    & =
    \frac{1}{k} \frac{1}{2d} \sum_{u,v \in V} a_{uv} \norm{F(u) - F(v)}^2 \label{eq:RLF}
	\\
	& =
    \frac{1}{k} \frac{1}{2d} \sum_{u,v \in V} a_{uv} \sum_{1 \leq i \leq k} 
    (f_i(u) - f_i(v))^2 \nonumber
	\\
    & =
    \frac{1}{k} \sum_{1 \leq i \leq k} \frac{1}{2d} \sum_{u,v \in V} a_{uv} 
    (f_i(u) - f_i(v))^2. \nonumber
\end{align}
Using \cref{eq:Lf.f} and the fact that each $f_i$ has norm one, it follows that
\begin{equation} \label{eq:RLF-avg-RLf}
    R_L(F) = \frac{1}{k} \sum_{1 \leq i \leq k} R_L(f_i).
\end{equation} 

Henceforth, we assume that the functions $f_1, f_2, \dots, f_k$,
using which the function $F$ is defined, are orthonormal.

\subsection{Preparatory lemmas}

We first state the lemmas that we will use, and come to their proofs later.

\begin{lemma} \label{lem:normFdist}
    For any $u, v \in V$ with $F(u) \neq 0$ and $F(v) \neq 0$, we have
    \begin{equation*}
        \norm{F(v)} dist(u,v) \leq 2 \norm{F(u) - F(v)}.
    \end{equation*}
\end{lemma}

\begin{lemma}[$F$ ``spreads out'' vertices across $\mathbb{R}^k$] \label{lem:spreadingF}
    Given any unit vector $\mathbf{w} \in \mathbb{R}^k$, we have
    \begin{equation*}
        \sum_{v \in V} \ip{F(v)}{\mathbf{w}}^2 = 1.
    \end{equation*}  
\end{lemma}

Given any subset $A$ of $V$, we call the quantity $\sum_{v \in A} \norm{F(v)}^2$
the \emph{mass} of a set $A$. Note that the mass of the set $V$ equals  $k$. 
For a nonempty subset $R$ of the unit sphere in $\mathbb{R}^k$, the \emph{diameter}
of $R$ is given by $diam(R) \coloneq \sup_{\mathbf{w}, \mathbf{z} \in R} 
\norm{\mathbf{w} - \mathbf{z}}$ and the set $V(R)$ is defined as
\begin{equation*}
    V(R) \coloneq \left\{ v \in V : F(v) \neq 0, \frac{F(v)}{\norm{F(v)}} \in R\right\}.
\end{equation*}

\begin{lemma}[If $R$ has ``small'' diameter, then $V(R)$ has ``small'' mass]
\label{lem:smallR-smallVR}
    Any nonempty subset $R$ of the unit sphere in $\mathbb{R}^k$, with $diam(R) < \sqrt{2}$,
    satisfies the inequality
    \begin{equation*}
        \sum_{v \in V(R)} \norm{F(v)}^2 \leq \left(1 - \frac{1}{2} diam(R)^2\right)^{-2}.        
    \end{equation*} 
\end{lemma}

\begin{lemma}[Well-separated sets each with ``small'' mass, but ``large'' total mass]
\label{lem:small-separated}
    There exist disjoint subsets $T_1, \dots, T_m$ of $V$ satisfying the following conditions.
    \begin{enumerate}[label=(\alph*)]
        \item $\sum_{i=1}^{m} \sum_{v \in T_i} \norm{F(v)}^2 \geq k - \frac{1}{4}$,
        \item If $u \in T_i$ and $v \in T_j$ with $i \neq j$, then $dist(u,v) \geq \Omega(k^{-3})$,
        \item For every $1 \leq i \leq m$, we have $\sum_{v \in T_i} \norm{F(v)}^2 
        \leq 1 + \frac{1}{4k}$.
    \end{enumerate}
\end{lemma}

\begin{lemma}[Well-separated sets each with ``large'' mass] \label{lem:large-separated}
    There exist $k$ disjoint subsets $A_1, \dots, A_k$ of $V$ satisfying the following conditions.
    \begin{enumerate}[label=(\alph*)]
        \item For every $1 \leq i \leq k$, we have $\sum_{v \in A_i} \norm{F(v)}^2 \geq \frac{1}{2}$,
        \item If $u \in A_i$ and $v \in A_j$ with $i \neq j$, then $dist(u,v) \geq \Omega(k^{-3})$.
    \end{enumerate} 
\end{lemma}

\begin{lemma}[Localization] \label{lem:localization}
    Let $A_1, \dots, A_t$ be subsets of $V$ such that for every $i \in \{1, \dots, t\}$, 
    we have $\sum_{v \in A_i} \norm{F(v)}^2 \geq \frac{1}{2}$, and there is a 
    real number $\delta \in (0,1]$ such that if $u \in A_i$ and $v \in A_j$ 
    with $i \neq j$, then $dist(u,v) \geq \delta$. Then there exist $t$ disjointly supported
    nonzero functions $g_1, \dots, g_t \in \ell^2(V)$ such that for every 
    $i \in \{1, \dots, t\}$, the following inequality is satisfied.
    \begin{equation*}
        R_L(g_i) \leq O(k \delta^{-2}) R_L(F).
    \end{equation*}
\end{lemma}

\begin{lemma} \label{lem:main-lemma}
    Let $f_1, \dots, f_k$ be orthonormal functions in $\ell^2(V)$. Then there exist
    disjointly supported nonzero functions $g_1, \dots, g_k \in \ell^2(V)$
    such that for every $i \in \{1, \dots, k\}$, we have
    \begin{equation*}
        R_L(g_i) \leq O(k^7) \max_{1 \leq j \leq k} R_L(f_j).
    \end{equation*}
\end{lemma}

\begin{lemma} \label{lem:expansion-in-support}
    Given any nonzero function $g \in \ell^2(V)$, there is a subset $S$
    of its support such that $\phi(S) \leq \sqrt{2R_L(g)}$.
\end{lemma}

\subsection{Proofs of the lemmas}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[node distance=2cm]
    
        \node (lem33) [lemmabox] {\cref{lem:spreadingF}};
        \node (lem34) [lemmabox, below of = lem33] {\cref{lem:smallR-smallVR}};
        \node (lem35) [lemmabox, below of = lem34] {\cref{lem:small-separated}};
        \node (lem36) [lemmabox, below of = lem35] {\cref{lem:large-separated}};
        \node (lem32) [lemmabox, right of = lem35, xshift=4cm] {\cref{lem:normFdist}};
        \node (lem37) [lemmabox, below of = lem32] {\cref{lem:localization}};
        \node (lem38) [lemmabox, below of = lem36, xshift=3cm] {\cref{lem:main-lemma}};
        \node (lem39) [lemmabox, below of = lem37, xshift=3cm] {\cref{lem:expansion-in-support}};
        \node (thm31) [lemmabox, below of = lem38, xshift=3cm] {\cref{thm:cheeger-ineq}};
    
        \draw [arrow] (lem33) -- (lem34);
        \draw [arrow] (lem34) -- (lem35);
        \draw [arrow] (lem35) -- (lem36);
        \draw [arrow] (lem32) -- (lem37);
        \draw [arrow] (lem36) -- (lem38);
        \draw [arrow] (lem37) -- (lem38);
        \draw [arrow] (lem38) -- (thm31);
        \draw [arrow] (lem39) -- (thm31);
    
    \end{tikzpicture}
    \caption{\label{fig:HOCIProofSketch} Proof sketch of \cref{thm:cheeger-ineq}}
\end{figure}

\begin{proof}[Proof of \cref{lem:normFdist}]
    Let $u, v \in V$ be arbitrary with $F(u) \neq 0$ and $F(v) \neq 0$. Then, we have
    \begin{align*}
        \norm{F(v)} dist(u,v) 
        & = 
        \norm{F(v)} \norm{\frac{F(u)}{\norm{F(u)}} - \frac{F(v)}{\norm{F(v)}}}
        \\
        & =
        \norm{F(u) \frac{\norm{F(v)}}{\norm{F(u)}} - F(v)}
        \\
        & =
        \norm{F(u) \frac{\norm{F(v)}}{\norm{F(u)}} - F(u) + F(u) -  F(v)}
        \\
        & \leq 
        \norm{F(u) \frac{\norm{F(v)}}{\norm{F(u)}} - F(u)} + \norm{F(u) -  F(v)}
        \\
        & =
        \norm{\left(\frac{\norm{F(v)}}{\norm{F(u)}} - 1\right) F(u)} + \norm{F(u) -  F(v)}
        \\
        & =
        \abs{\frac{\norm{F(v)}}{\norm{F(u)}} - 1} \norm{F(u)} + \norm{F(u) -  F(v)}
        \\
        & =
        \abs{\norm{F(v)} - \norm{F(u)}} + \norm{F(u) -  F(v)}
        \\
        & \leq
        2 \norm{F(u) -  F(v)},
    \end{align*}
    where the last inequality follows from the fact that for any 
    $\mathbf{w}, \mathbf{z} \in \mathbb{R}^k$, the inequality 
    $\abs{\norm{\mathbf{w}} - \norm{\mathbf{z}}} \leq \norm{\mathbf{w} - \mathbf{z}}$ holds.
\end{proof}

\begin{proof}[Proof of \cref{lem:spreadingF}]
    Define a linear map $\map{U}{\mathbb{R}^k}{\ell^2(V)}$ by
    \begin{equation*}
        (U \mathbf{w})(v) \coloneq \ip{F(v)}{w},
    \end{equation*}  
    for every $\mathbf{w} \in \mathbb{R}^k$ and $v \in V$.
    Observe that the transpose $\map{U^\mathrm{t}}{\ell^2(V)}{\mathbb{R}^k}$ of 
    the linear map $U$ is given by
    \begin{equation*}
        U^\mathrm{t} f = (\ip{f_1}{f}, \dots, \ip{f_k}{f}),
    \end{equation*}
    for every $f \in \ell^2(V)$. For $i \in \{1, \dots, k\}$, let $e_i$ denote 
    the standard basis vectors in $\mathbb{R}^k$. Then, for each $i$, note that 
    $U^\mathrm{t}U e_i = U^\mathrm{t} f_i = e_i$, since the functions $f_1, \dots, f_k$ 
    are orthonormal. This shows that the map $U^\mathrm{t}U$ is the identity
    operator on $\mathbb{R}^k$. Hence, for any unit vector $\mathbf{w} \in \mathbb{R}^k$, 
    we have
    \begin{equation*}
        \sum_{v \in V} \ip{F(v)}{\mathbf{w}}^2 = \norm{U \mathbf{w}}^2 
        = \ip{U^\mathrm{t}U \mathbf{w}}{\mathbf{w}} = \ip{\mathbf{w}}{\mathbf{w}} = 1. 
    \end{equation*}
\end{proof}

For each $v \in V$ with $F(v) \neq 0$, define 
\begin{equation*}
    \bar{F}(v) \coloneq \frac{F(v)}{\norm{F(v)}}. 
\end{equation*}

\begin{proof}[Proof of \cref{lem:smallR-smallVR}]
    Let $R$ be a nonempty subset of the unit sphere in $\mathbb{R}^k$ with 
    $diam(R) < \sqrt{2}$ and $\mathbf{w}$ be a vector in $R$. Then, for any $v \in V(R)$,
    we have $\norm{\bar{F}(v) - \mathbf{w}} \leq diam(R)$. Thus, we obtain
    \begin{align*}
        diam(R)^2 \geq \norm{\bar{F}(v) - \mathbf{w}}^2 
        & = 
		\norm{\bar{F}(v)}^2 + \norm{\mathbf{w}}^2 - 2 \ip{\bar{F}(v)}{\mathbf{w}}
		\\
        & = 
		2 - 2 \ip{\bar{F}(v)}{\mathbf{w}},
    \end{align*}
    which implies 
    \begin{equation*}
        \ip{\bar{F}(v)}{\mathbf{w}} \geq 1 - \frac{1}{2} diam(R)^2,
    \end{equation*}
    and since $diam(R) < \sqrt{2}$, we conclude that  
    \begin{equation*}
        \frac{1}{\norm{F(v)}^2} \ip{F(v)}{\mathbf{w}}^2 
        = \ip{\bar{F}(v)}{\mathbf{w}}^2 \geq \left(1 - \frac{1}{2} diam(R)^2\right)^2.
    \end{equation*} 
    As this is true for every $v \in V(R)$, we get
    \begin{equation*}
        \sum_{v \in V(R)} \ip{F(v)}{\mathbf{w}}^2 \geq \left(1 - \frac{1}{2} diam(R)^2\right)^2
        \sum_{v \in V(R)} \norm{F(v)}^2.
    \end{equation*} 
    We arrive at the desired result using the inequality
    $\sum_{v \in V(R)} \ip{F(v)}{\mathbf{w}}^2 \leq 1$, 
	which is obtained as a consequence of \cref{lem:spreadingF}.
    \end{proof}

\begin{proof}[Proof of \cref{lem:small-separated}]
    Set $L = \frac{1}{\sqrt{5}k}$. We tile $\mathbb{R}^k$ with cubes of the form
    $\prod_{i=1}^{k} [n_iL, n_iL+L)$, where $n_i$ is an integer for every $i$.
    Each of these cubes has diameter equal to $\frac{1}{\sqrt{5k}}$. For every cube 
    $C = \prod_{i=1}^{k} [n_iL, n_iL+L)$, define its \emph{core} to be the cube
    \begin{equation*}
        \tilde C = \prod_{i=1}^{k} \left[n_iL + \frac{L}{8k^2}, n_iL + L - \frac{L}{8k^2}\right).
    \end{equation*}
    Note that the distance between any two points in the cores of two different
    cubes is at least $\frac{L}{4k^2}$, which equals $\frac{1}{4\sqrt{5}k^3}$.
	Let $\tilde C_w$ denote the cube when the core $\tilde C$ of a cube $C$ (as above), 
    is shifted by $w \in \mathbb{R}^k$, that is, if $w = (w_1, \dots, w_k)$, 
    then we have
	\begin{equation*}
		\tilde C_w = \prod_{i=1}^{k} \left[n_iL + \frac{L}{8k^2} + w_i, 
    	n_iL + L - \frac{L}{8k^2} + w_i\right).
	\end{equation*}

    Let $\Omega = [0,L)^k$ be a probability space equipped with the Borel $\sigma$-algebra
    and the probability measure $\mathbb{P}$ defined as follows. For Borel-measurable 
    subsets $I_1, \dots, I_k$ of $[0,L)$, define 
    \begin{equation*}
        \mathbb{P}(I_1 \times \dots \times I_k) \coloneq 
        \frac{m(I_1) \times \dots \times m(I_k)}{L^k},
    \end{equation*}
    where $m(B)$ denotes the Borel measure of a subset $B$ of $[0,L)$.
    Fix any $v \in V$ with $F(v) \neq 0$ and let $\bar{F}(v) = (z_1, \dots, z_k)$. 
    Consider the set $A_v$ defined as follows.
    \begin{equation*}
        A_v \coloneq \{w \in \Omega : \bar{F}(v) \in \tilde C_w \text{ for some cube } 
        C \text{ (of the above form)}\}.
    \end{equation*}
    Observe that 
    \begin{align*}
        A_v 
        & = 
        \left\{(w_1, \dots, w_k) \in \Omega \biggm| 
        \begin{array}{l} 
            \forall\ 1 \leq i \leq k, \exists\ n_i \in \mathbb{Z} \text{ such that} 
			\\
            z_i \in \left[n_iL + \frac{L}{8k^2} + w_i, n_iL + L - \frac{L}{8k^2} + w_i\right)
        \end{array}
        \right\} 
        \\
        & = 
        \left\{(w_1, \dots, w_k) \in \Omega \biggm| 
        \begin{array}{l} 
            \forall\ 1 \leq i \leq k, \exists\ n_i \in \mathbb{Z} \text{ such that} 
			\\
            w_i \in \left(z_i - n_iL - L + \frac{L}{8k^2}, z_i - n_iL - \frac{L}{8k^2}\right]
        \end{array}
        \right\}
        \\
        & =
        \prod_{i=1}^{k} \left(\left(\bigcup_{n_i \in \mathbb{Z}} 
        \left(z_i - n_iL - L + \frac{L}{8k^2}, z_i - n_iL - \frac{L}{8k^2}\right]\right) 
        \cap [0,L)\right).
    \end{align*}
    For every $i \in \{1, \dots, k\}$, note that there is a unique integer $n_i$ such that 
    $z_i - n_iL - L + \frac{L}{8k^2}$ lies in $[0,L)$, and there is a unique integer $m_i$
    such that $z_i - m_iL - \frac{L}{8k^2}$ belongs to the interval $[0,L)$. 
    Also, these are the only cases when the sets $J_{r_i} \coloneq 
    \left(z_i - r_iL - L + \frac{L}{8k^2}, z_i - r_iL - \frac{L}{8k^2}\right] \cap [0,L)$, 
    for $r_i \in \mathbb{Z}, 1 \leq i \leq k$, are nonempty. So, we have
    \begin{align*}
        A_v & =
        \prod_{i=1}^{k} \left(\left(\left(z_i - n_iL - L + \frac{L}{8k^2}, 
        z_i - n_iL - \frac{L}{8k^2}\right] \cap [0,L)\right)\right.
        \\
        & \hspace{1.2cm} 
        \bigcup \left.\left(\left(z_i - m_iL - L + \frac{L}{8k^2}, 
        z_i - m_iL - \frac{L}{8k^2}\right] \cap [0,L)\right)\right)
        \\
        & =
        \prod_{i=1}^{k} (J_{n_i} \cup J_{m_i}).
    \end{align*}
    Let $i \in \{1, \dots, k\}$ be arbitrary. If $n_i = m_i$, then we get 
    $J_{n_i} \cup J_{m_i} = J_{n_i}$, and 
    \begin{equation*}
        m(J_{n_i} \cup J_{m_i}) = z_i - n_iL - \frac{L}{8k^2} - z_i + n_iL + L - \frac{L}{8k^2} 
        = L - \frac{L}{4k^2}.
    \end{equation*}
    On the other hand, 
    if $n_i \neq m_i$, then we obtain $J_{n_i} = \left(z_i - n_iL - L + \frac{L}{8k^2},
    L\right)$ and $J_{m_i} = \left[0, z_i - m_iL - \frac{L}{8k^2}\right]$ with
    $m_i = n_i + 1$, and the intervals $J_{n_i}$ and $J_{m_i}$ are disjoint. 
    Hence, it follows that
    \begin{equation*}
        m(J_{n_i} \cup J_{m_i}) 
        = z_i - n_iL - L - \frac{L}{8k^2} + L - z_i + n_iL + L - \frac{L}{8k^2}
        = L - \frac{L}{4k^2}.
    \end{equation*}
    Thus, in any case, using Bernoulli's inequality, we have
    \begin{equation*}
        \mathbb{P}(A_v) 
        = \frac{1}{L^k} \prod_{i=1}^k \left(L - \frac{L}{4k^2}\right)
        = \left(1 - \frac{1}{4k^2}\right)^k \geq 1 - \frac{k}{4k^2}
    	= 1 - \frac{1}{4k}.
    \end{equation*}
    Note that this is true for every $v \in V$ with $F(v) \neq 0$.
    
    Now for each $w \in \Omega$, let $R_w \coloneq \left(\bigcup_{C} \tilde C_w\right) 
    \cap \mathbb{S}^{k-1}$, where $\mathbb{S}^{k-1}$ denotes the unit sphere 
    in $\mathbb{R}^k$. For every $v \in V$, define random variables 
    $\map{X, X_v}{\Omega}{\mathbb{R}}$ by
    \begin{equation*}
        X(w) = \sum_{v \in V(R_w)} \norm{F(v)}^2, 
    \end{equation*}
    and
    \begin{equation*}
        X_v(w) = \begin{cases}
            \norm{F(v)}^2 1_{A_v}(w) 
            & \text{if } F(v) \neq 0,
            \\
            0
            & \text{if } F(v) = 0,
        \end{cases}
    \end{equation*}
    for every $w \in \Omega$. Then, note that $X = \sum_{v \in V} X_v$, and thus,
    we have
    \begin{align*}
        \mathbb{E}[X] = \sum_{v \in V} \mathbb{E}[X_v]
        & =
        \sum_{v \in V : F(v) \neq 0} \norm{F(v)}^2 \mathbb{P}(A_v)
        \\
        & \geq
        \left(1 - \frac{1}{4k}\right) \sum_{v \in V} \norm{F(v)}^2
        \\
        & =
        \left(1 - \frac{1}{4k}\right) \sum_{v \in V} \sum_{i=1}^k f_i(v)^2
        \\
        & =
        \left(1 - \frac{1}{4k}\right) k
        \\
        & =
        k - \frac{1}{4}.
    \end{align*}
    Now choose a point $w \in \Omega$ such that $X(w) \geq k - \frac{1}{4}$.
    (The existence of such a point is guaranteed by the pigeonhole principle
    (see \cite{Prendiville-Fourier-in-Ramsey}*{Proposition 1.13} for instance.))
    Consider those \emph{$w$-shifted cores} $\tilde C_w$ whose intersection 
    with the unit sphere is nonempty, and call those intersections $R_1, \dots, R_m$. 
    Then, we get
    \begin{equation*}
        X(w) = \sum_{i=1}^{m} \sum_{v \in V(R_i)} \norm{F(v)}^2 \geq k - \frac{1}{4}.
    \end{equation*}

    Also, if $u \in V(R_i)$ and $v \in V(R_j)$ with $i \neq j$, then
	\begin{equation*}
		dist(u,v) = \norm{\bar{F}(u) - \bar{F}(v)}, 
	\end{equation*} 
	and since $\bar{F}(u) \in R_i$ and $\bar{F}(v) \in R_j$,
    we have $dist(u,v) \geq \frac{1}{4\sqrt{5}k^3}$.

    Further, since each $R_i$ has diameter at most $\frac{1}{\sqrt{5k}}$, using 
    \cref{lem:smallR-smallVR}, for every $i \in \{1, \dots, m\}$, we have
    \begin{equation*}
        \sum_{v \in V(R_i)} \norm{F(v)}^2 \leq \left(1 - \frac{1}{10k}\right)^{-2}
        \leq \frac{1}{1 - \frac{1}{5k}} = \frac{5k}{5k-1} = 1 + \frac{1}{5k-1}
        \leq 1 + \frac{1}{4k}.
    \end{equation*} 
	Therefore, the subsets $T_i \coloneq V(R_i)$, for $i \in \{1, \dots, m\}$, serve our purpose.
\end{proof}

\begin{proof}[Proof of \cref{lem:large-separated}]
    Consider subsets $T_1, \dots, T_m$ of $V$ as guaranteed by \cref{lem:small-separated},
    and keep on merging two subsets whenever each of those subsets has mass less than 
    $\frac{1}{2}$. Run this procedure on \emph{new} sets as well, till at most one set 
    having mass less than $\frac{1}{2}$ is left. Note that we get at least one set 
    having mass $\geq \frac{1}{2}$ at the end of this process, since the above procedure 
    keeps the \emph{total} mass unchanged and the total mass is at least $\frac{3}{4}$. 
    We enumerate the subsets with mass $\geq \frac{1}{2}$, 
    left after the procedure is terminated, by $A_1, \dots, A_t$ for some $t \geq 1$. 

    For every $i$, since the mass of the subset $T_i$ is not more than $1 + \frac{1}{4k}$, 
    and the mass of each of the \emph{new} sets among $A_1, \dots, A_t$ is less than $1$ 
    (as they are formed by merging two sets each of mass less than $\frac{1}{2}$), 
    the total mass of the sets is at most $\frac{1}{2} + t \left(1 + \frac{1}{4k}\right)$.
    Now if $t$ is less than $k$, then we have
    \begin{equation*}
        \sum_{i=1}^{m} \sum_{v \in T_i} \norm{F(v)}^2 
        \leq \frac{1}{2} + (k-1) \left(1 + \frac{1}{4k}\right)
        = \frac{1}{2} + k - 1 + \frac{1}{4} - \frac{1}{4k} < k - \frac{1}{4},
    \end{equation*}
    which contradicts \cref{lem:small-separated}, and hence, we conclude that $t \geq k$.
    Hence, the sets $A_1, \dots, A_k$ are as required.
\end{proof}

\begin{proof}[Proof of \cref{lem:localization}]
    Let the subsets $A_1, \dots, A_t$ of $V$ and a real number $\delta > 0$ be as in 
    the statement of the lemma. For every $i \in \{1, \dots, t\}$, define the 
    \emph{smooth indicator function} $\tau_i$ of the subset $A_i$ at any $v \in V$ as follows.
    \begin{equation*}
        \tau_i(v) =
        \begin{cases}
            0
            & \text{if } dist(v, A_i) \geq \frac{\delta}{2},
            \\
            1 - \frac{2}{\delta} dist(v, A_i)
            & \text{otherwise}.
        \end{cases}
    \end{equation*}
    For every $i$, define a function $g_i \in \ell^2(V)$ at any $v \in V$ by
    \begin{equation*}
        g_i(v) \coloneq \tau_i(v) \norm{F(v)}.
    \end{equation*} 
    We now show that the functions $g_1, \dots, g_t$ are disjointly supported.
    Suppose that there are indices $i, j$ and an element $v \in V$ such that 
    $g_i(v)$ and $g_j(v)$ are nonzero. Then the functions $\tau_i$ and $\tau_j$
    are also nonzero at $v$, which implies that $dist(v, A_i) < \frac{\delta}{2}$ 
    and $dist(v, A_j) < \frac{\delta}{2}$. Hence, there exist elements $u_i \in A_i$
    and $u_j \in A_j$ such that $dist(v, u_i) < \frac{\delta}{2}$ and $dist(v, u_j) 
    < \frac{\delta}{2}$. Now it follows from the triangle inequality for the pseudo-metric 
    $dist$ that
    \begin{equation*}
        dist(u_i, u_j) \leq dist(u_i, v) + dist(v, u_j) < \frac{\delta}{2} + \frac{\delta}{2} = \delta,
    \end{equation*}
    and then the hypothesis forces $i$ to be equal to $j$. Thus, the functions
    $g_1, \dots, g_t$ are disjointly supported. 

    Moreover, for every $i \in \{1, \dots, t\}$, we have
    \begin{equation*}
        \ip{g_i}{g_i} = \sum_{v \in V} \tau_i^2(v) \norm{F(v)}^2 
        \geq \sum_{v \in A_i} \tau_i^2(v) \norm{F(v)}^2 = \sum_{v \in A_i} \norm{F(v)}^2
        \geq \frac{1}{2}.
    \end{equation*}
    In particular, each $g_i$ is a nonzero function and its Rayleigh quotient is well-defined. 
    
    Now fix an arbitrary element $i$ from the set $\{1, \dots, t\}$. 
    For any $u, v \in V$, we will prove that 
    $\abs{g_i(v) - g_i(u)} \leq \norm{F(v) - F(u)} \left(1 + \frac{4}{\delta}\right)$.
    If $F(u)$ or $F(v)$ is zero, then this inequality holds trivially using the fact
    that $\tau_i(w) \leq 1$ for all $w \in V$. 
	Assume now that both $F(u)$ and $F(v)$ are nonzero. Note that
    \begin{align*}
        & \hspace{0.49cm} \abs{g_i(v) - g_i(u)} 
		\\
        & =
        \abs{\tau_i(v) \norm{F(v)} - \tau_i(u) \norm{F(u)}}
        \\
        & =
        \abs{\tau_i(v) \norm{F(v)} - \tau_i(v) \norm{F(u)} + \tau_i(v) \norm{F(u)} 
        - \tau_i(u) \norm{F(u)}}
        \\
        & \leq 
        \abs{\tau_i(v) \norm{F(v)} - \tau_i(v) \norm{F(u)}} 
        + \abs{\tau_i(v) \norm{F(u)} - \tau_i(u) \norm{F(u)}}
        \\
        & =
        \tau_i(v) \abs{\norm{F(v)} - \norm{F(u)}} + \norm{F(u)} \abs{\tau_i(v) - \tau_i(u)}
        \\
        & \leq
        \norm{F(v) - F(u)} + \norm{F(u)} \abs{\tau_i(v) - \tau_i(u)}.
    \end{align*}
    We claim that 
    $\abs{\tau_i(v) - \tau_i(u)} \leq \frac{2}{\delta} \abs{dist(v, A_i) - dist(u, A_i)}$. 
    If we have the inequalities $dist(v, A_i) \geq \frac{\delta}{2}$ and 
    $dist(u, A_i) \geq \frac{\delta}{2}$, then clearly the claim holds. On the other hand,
    if $dist(v, A_i) < \frac{\delta}{2}$ and $dist(u, A_i) < \frac{\delta}{2}$, then 
    observe that
    \begin{align*}
        \abs{\tau_i(v) - \tau_i(u)} 
		& = 
		\abs{\left(1 - \frac{2}{\delta} dist(v, A_i)\right) 
        - \left(1 - \frac{2}{\delta} dist(u, A_i)\right)} 
		\\
        & = 
		\frac{2}{\delta} \abs{dist(v, A_i) - dist(u, A_i)}.
    \end{align*}
    Otherwise, if $dist(u, A_i) < \frac{\delta}{2}$ and $dist(v, A_i) \geq \frac{\delta}{2}$,
    then we have
    \begin{align*}
        \abs{\tau_i(v) - \tau_i(u)} 
		& = 
		1 - \frac{2}{\delta} dist(u, A_i)
		\\
        & \leq 
		\frac{2}{\delta} dist(v, A_i) - \frac{2}{\delta} dist(u, A_i)
		\\
        & = 
		\frac{2}{\delta} \abs{dist(v, A_i) - dist(u, A_i)},
    \end{align*}
    and similarly, the claim holds if $dist(v, A_i) < \frac{\delta}{2}$ and 
    $dist(u, A_i) \geq \frac{\delta}{2}$. Then using the triangle inequality 
    for $dist$, it follows that $\abs{\tau_i(v) - \tau_i(u)} \leq \frac{2}{\delta} dist(v,u)$.

    Hence, for any $u,v \in V$, using \cref{lem:normFdist}, we obtain
    \begin{align*}
        \abs{g_i(v) - g_i(u)} 
		& \leq 
		\norm{F(v) - F(u)} + \frac{2}{\delta} \norm{F(u)} dist(v,u)
		\\
        & \leq 
		\norm{F(v) - F(u)} \left(1 + \frac{4}{\delta}\right),
    \end{align*}
    and thus, the numerator of the Rayleigh quotient of $g_i$ is
    \begin{align*}
        \ip{Lg_i}{g_i} 
        & = 
        \frac{1}{2d} \sum_{u,v \in V} a_{uv} (g_i(u) - g_i(v))^2 \tag{using \cref{eq:Lf.f}}
        \\
        & \leq
        \left(1 + \frac{4}{\delta}\right)^2 \frac{1}{2d} \sum_{u,v \in V} a_{uv} \norm{F(v) - F(u)}^2
        \\
        & =
        \left(1 + \frac{4}{\delta}\right)^2 k R_L(F) \tag{using \cref{eq:RLF}}.
    \end{align*} 
    This proves that the Rayleigh quotient of $g_i$ is
    \begin{align*}
        R_L(g_i) = \frac{\ip{Lg_i}{g_i}}{\ip{g_i}{g_i}} 
        & \leq 
		2 \left(1 + \frac{4}{\delta}\right)^2 k R_L(F)
		\\
        & = 
		2 \left(1 + \frac{8}{\delta} + \frac{16}{\delta^2}\right) k R_L(F)
		\\
        & \leq 
		50 \frac{k}{\delta^2} R_L(F),
    \end{align*}
    where we have used the fact that $\delta$ lies in the interval $(0,1]$ 
    in the last inequality.
\end{proof}

\begin{proof}[Proof of \cref{lem:main-lemma}]
	From the proof of \cref{lem:small-separated} and \cref{lem:large-separated}, note that 
	$\delta = \frac{1}{4 \sqrt{5} k^3}$ works in the statement of \cref{lem:localization}.
    Combining this fact with \cref{lem:large-separated} and \cref{lem:localization}, 
    we obtain disjointly supported nonzero functions $g_1, \dots, g_k \in \ell^2(V)$ 
    such that the inequality $R_L(g_i) \leq O(k^7) R_L(F)$ holds for every 
    $i \in \{1, \dots, k\}$. The desired inequality follows from \cref{eq:RLF-avg-RLf}
    and the fact that the average of $k$ real numbers is at most as large as their maximum.
\end{proof}

\begin{proof}[Proof of \cref{lem:expansion-in-support}]
    Let $\map{g}{V}{\mathbb{R}}$ be a nonzero function. We will prove that 
    there is a real number $t_0 \in [0, \max_{u  \in V} g(u)^2)$ such that the inequality
    \begin{equation*}
        \phi(\{v \in V \mid g(v)^2 > t_0\}) \leq \sqrt{2 R_L(g)}
    \end{equation*}
    holds. It is enough to prove this, as the set $\{v \in V \mid g(v)^2 > t\}$ is 
    a nonempty subset of the support of $g$ for every $t \in [0, \max_{u  \in V} g(u)^2)$.

    Let us denote the number $\max_{u  \in V} g(u)^2$ by $M$, and for any $t \in [0,M)$, 
    denote by $S_t$ the set $\{v \in V \mid g(v)^2 > t\}$. Note that
    \begin{equation*}
        \int_{0}^{M} d \abs{S_t}\, \mathrm{d}t 
        = d \sum_{v \in V} \int_{0}^{M} 1_{S_t}(v)\, \mathrm{d}t
        = d \sum_{v \in V} \int_{0}^{g(v)^2} 1\, \mathrm{d}t
        = d \sum_{v \in V} g(v)^2,
    \end{equation*}
    and that
    \begin{align*}
        & \hspace{0.49cm}
		\int_{0}^{M} \ip{T1_{V \setminus S_t}}{1_{S_t}}\, \mathrm{d}t
		\\
        & =
        \sum_{u \in V} \sum_{v \in V} \int_{0}^{M} 
        a_{uv} 1_{S_t}(u) 1_{V \setminus S_t}(v)\, \mathrm{d}t
        \\
        & =
        \sum_{\substack{u,v \in V \\ g(v)^2 < g(u)^2}} \int_{g(v)^2}^{g(u)^2} a_{uv}\, \mathrm{d}t
        \\
        & =
        \sum_{\substack{u,v \in V \\ g(v)^2 < g(u)^2}} a_{uv} (g(u)^2 - g(v)^2)
        \\
        & =
        \frac{1}{2} \left(\sum_{\substack{u,v \in V \\ g(v)^2 < g(u)^2}} a_{uv} (g(u)^2 - g(v)^2)
        + \sum_{\substack{u,v \in V \\ g(u)^2 < g(v)^2}} a_{vu} (g(v)^2 - g(u)^2)\right)
        \\
        & =
        \frac{1}{2} \sum_{u,v \in V} a_{uv} \abs{g(u)^2 - g(v)^2} 
        \tag{since $a_{uv} = a_{vu}$ for all $u, v \in V$}
        \\
        & =
        \frac{1}{2} \sum_{u,v \in V} a_{uv} \abs{g(u) - g(v)} \abs{g(u) + g(v)}
        \\
        & \leq
        \frac{1}{2} \left(\sum_{u,v \in V} a_{uv} (g(u) - g(v))^2\right)^\frac{1}{2}
        \left(\sum_{u,v \in V} a_{uv} (g(u) + g(v))^2\right)^\frac{1}{2}
        \tag{using the Cauchy--Schwarz inequality}
        \\
        & \leq
        \frac{1}{2} \left(\sum_{u,v \in V} a_{uv} (g(u) - g(v))^2\right)^\frac{1}{2}
        \left(\sum_{u,v \in V} a_{uv} (2g(u)^2 + 2g(v)^2)\right)^\frac{1}{2}
        \\
        & =
        \frac{\sqrt{2}}{2} \left(\sum_{u,v \in V} a_{uv} (g(u) - g(v))^2\right)^\frac{1}{2}
        \left(2d \sum_{v \in V} g(v)^2\right)^\frac{1}{2},
    \end{align*}
    and thus, we have
    \begin{equation*}
        \frac{\int_{0}^{M} \ip{T1_{V \setminus S_t}}{1_{S_t}}\, \mathrm{d}t}
        {\int_{0}^{M} d \abs{S_t}\, \mathrm{d}t}
        \leq \frac{\left(\sum_{u,v \in V} a_{uv} (g(u) - g(v))^2\right)^\frac{1}{2}}
        {\left(d \sum_{v \in V} g(v)^2\right)^\frac{1}{2}} = \sqrt{2R_L(g)}.
    \end{equation*}
    Now using \cref{lemma:f/g&intf/intg}, we conclude that there exists 
	a real number $t_0 \in [0,M)$ such that the following inequality holds.
    \begin{equation*}
        \frac{\ip{T1_{V \setminus S_{t_0}}}{1_{S_{t_0}}}}{d \abs{S_{t_0}}} 
        \leq \frac{\int_{0}^{M} \ip{T1_{V \setminus S_t}}{1_{S_t}}\, \mathrm{d}t}
        {\int_{0}^{M} d \abs{S_t}\, \mathrm{d}t} \leq \sqrt{2R_L(g)}.
    \end{equation*}
    Therefore, the subset $S_{t_0}$ of the support of the function $g$ satisfies
    the inequality $\phi(S_{t_0}) \leq \sqrt{2R_L(g)}$.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:cheeger-ineq}}

\begin{proof}[Proof of \cref{thm:cheeger-ineq}]
    Let $f_1, \dots, f_k$ be orthonormal eigenfunctions corresponding to the eigenvalues
    $\lambda_1, \dots, \lambda_k$, respectively, of the operator $L$. Then 
    \cref{lem:main-lemma} guarantees the existence of disjointly supported 
    nonzero functions $g_1, \dots, g_k \in \ell^2(V)$ such that for every 
    $i \in \{1, \dots, k\}$, we have
    \begin{equation*}
        R_L(g_i) \leq O(k^7) \max_{1 \leq j \leq k} R_L(f_j).
    \end{equation*}
    For every $j \in \{1, \dots, k\}$, note that $R_L(f_j) = \lambda_j$, and therefore,
    the Rayleigh quotient of each $g_i$ is at most $O(k^7) \lambda_k$. Now applying
    \cref{lem:expansion-in-support} to every nonzero function $g_i$, we get 
    a nonempty subset $S_i$ of its support such that $\phi(S_i) \leq \sqrt{2R_L(g_i)}
    \leq O(k^{3.5}) \sqrt{\lambda_k}$. Hence, we have nonempty disjoint subsets
    $S_1, \dots, S_k$ of $V$ such that 
    \begin{equation*}
        \max_{1 \leq i \leq k} \phi(S_i) \leq O(k^{3.5}) \sqrt{\lambda_k}.
    \end{equation*}
    Now the desired inequality follows from the above inequality and 
    the very definition of the $k$-way expansion constant of $T$.
\end{proof}

\chapter{The dual Cheeger--Buser inequality for graphons}

Lov\'asz and his collaborators \cites{Lovasz-Szegedy-limitsofDenseGraphs06,
Lovasz-etal-Counthomo06,Lovasz-etal-cgtseqDenseGraphs08} developed 
the theory of graph limits, through both algebraic and analytic perspectives. 
They studied graphons and graphings, which arise as limits of convergent sequences 
of graphs and bounded degree graphs, respectively. 
Several results regarding graphons and graphings are discussed quite extensively 
in the book by Lov\'asz \cite{Lovasz-LargeNet12}. 
The theory of graph limits has found lots of connections with many other branches 
of mathematics, including extremal graph theory, probability theory, higher-order 
Fourier analysis, ergodic theory, number theory, group theory, representation theory, 
category theory, the limit theory of metric spaces, and numerous applications 
in other subjects like computer science, network theory and statistical physics.

Various notions in the context of graphs have been extended to graph limits, 
for instance, homomorphism densities \cite{Lovasz-Szegedy-limitsofDenseGraphs06},
Szemer\'edi's regularity lemma \cite{Lovasz-Szegedy-SzemerdiLemma07},
independent sets, cliques, and colorings \cite{IndSets-cliques-colorings20}, 
and tilings \cite{TilingsGraphons-21}.
Khetan and Mj \cite{Abhishek-Mahan24} established the analogs of the discrete
Cheeger--Buser inequality for graphs in the case of graphons and graphings. 
Given a connected graphon $W$, having Cheeger constant $h_W$ and 
the bottom of the spectrum of its Laplacian $\lambda_W$, they proved that
\begin{equation*}
    \frac{h_W^2}{8} \leq \lambda_W \leq 2 h_W.
\end{equation*}
They also showed that the Cheeger--Buser inequality for regular graphs 
can be recovered from this inequality for graphons. 

In this chapter, we introduce the notion of bipartiteness ratio in the context 
of graphons. Also, we establish the dual Cheeger--Buser inequality for graphons, 
which relates the gap between $2$ and the \emph{top of the spectrum} of the Laplacian 
of a graphon with its bipartiteness ratio. We have discussed the dual Cheeger--Buser 
inequality for graphs in \cref{ch:dualCBGraphs}. Our result is its analog for graphons.
We prove the following result obtained in the preprint \cite{Mugdha-duaCBGraphons25}.

\begin{thm}[The dual Cheeger--Buser inequality for graphons] \label{thm:main}
    Let $W$ be a connected graphon, $\beta_W$ denote its bipartiteness ratio and
    $\lambda_W^{\max}$ denote the top of the spectrum of its Laplacian. 
    Then the following inequality holds.
    \begin{equation*}
        \frac{\beta_W^2}{2} \leq 2 - \lambda_W^{\max} \leq 2 \beta_W.
    \end{equation*}
\end{thm}

\section{Preliminaries} \label{section:prelim}

In the following, by a measurable subset, we mean a Lebesgue measurable subset, 
and we denote the Lebesgue measure on $I = [0,1]$ by $\mu_L$.

A funcion $\map{W}{I^2}{I}$ is called a \emph{graphon} if $W$ is a Lebesgue measurable 
function which is symmetric, that is, $W(x,y) = W(y,x)$ for all $(x,y) \in I^2$. 
We say that a graphon $W$ is \emph{connected} if $\int_{A \times A^c} W > 0$
for every measurable subset $A$ of $I$ with $0 < \mu_L(A) < 1$.

Let $W$ be a connected graphon. For every measurable subset $A$ of $I$ and 
$S$ of $I^2$, define
\begin{equation*}
    \nu(A) \coloneq \int_{A \times I} W(x,y)\, \mathrm{d}x\, \mathrm{d}y, 
    \quad \text{ and } \quad
    \eta(S) \coloneq \int_{S} W(x,y)\, \mathrm{d}x\, \mathrm{d}y.
\end{equation*}
Then $\nu$ and $\eta$ are measures on $I$ and $I^2$, respectively. 
Note that the $\mathbb{R}$-vector space $L^2(I, \nu)$ is a Hilbert space with 
the inner product $\ip{\cdot}{\cdot}_v$, given by
\begin{equation*}
    \ip{f}{g}_v = \int_{I^2} f(x) g(x) W(x,y)\, \mathrm{d}y\, \mathrm{d}x,
\end{equation*}
for all $f,g \in L^2(I, \nu)$. We denote the restriction of the measure $\eta$
to the measurable subsets of the set $E = \{(x,y) \in I^2 : y > x\}$ also by $\eta$, 
and denote the inner product on the $\mathbb{R}$-Hilbert space $L^2(E, \eta)$
by $\ip{\cdot}{\cdot}_e$, which is given by
\begin{equation*}
    \ip{f}{g}_e = \int_0^1 \int_{x}^{1} f(x,y) g(x,y) W(x,y)\, \mathrm{d}y\, \mathrm{d}x,
\end{equation*}
for all $f, g \in L^2(E, \eta)$. The norms induced by the inner products 
$\ip{\cdot}{\cdot}_v$ and $\ip{\cdot}{\cdot}_e$ are denoted by $\norm[v]{\cdot}$
and $\norm[e]{\cdot}$, respectively.

Given any function $f \in L^2(I, \nu)$, define $df(x,y) \coloneq f(y) - f(x)$,
for all $(x,y) \in I^2$. Then it is proved in \cite{Abhishek-Mahan24}*{Lemma 3.3} that
the map $\map{d}{L^2(I, \nu)}{L^2(E, \eta)}$ which maps $f \in L^2(I, \nu)$ to 
the function $df|_E \in L^2(E, \eta)$ is a bounded linear operator. 
Let $\map{d^*}{L^2(E, \eta)}{L^2(I, \nu)}$ denote the adjoint of the operator $d$,
and define the \emph{Laplacian} $\Delta_W$ of $W$ by $\Delta_W = d^*d$, 
which is a bounded linear operator on the space $L^2(I, \nu)$. 

Given a graphon $W$, for all $x \in I$, the \emph{degree} of $x$ is defined by
\begin{equation*}
    d_W(x) \coloneq \int_I W(x,y)\, \mathrm{d}y.
\end{equation*}
If $W$ is a connected graphon, then $\eta(I^2) = \int_I d_W(x)\, \mathrm{d}x$ is positive,
and thus, $d_W$ is positive $\mu_L$-a.e. 
In that case, for every $f \in L^2(I,\nu)$ and $x \in I$ with $d_W(x) \neq 0$,
it is shown in \cite{Abhishek-Mahan24}*{Section 3.2} that
\begin{equation*}
    (\Delta_W f)(x) = f(x) - \frac{1}{d_W(x)} (T_W f)(x),
\end{equation*}
where the linear operator $\map{T_W}{L^2(I,\nu)}{L^2(I,\nu)}$ is defined by
\begin{equation*}
    (T_W f)(x) = \int_I W(x,y) f(y)\, \mathrm{d}y.
\end{equation*}
For the sake of brevity, we will write $\Delta_W = I - \frac{1}{d_W} T_W$,
where $I$ denotes the identity operator on $L^2(I,\nu)$, by abuse of notation.

\begin{defn}[Top of the spectrum]
    Given a conncted graphon $W$, the \emph{top of the spectrum} of its Laplacian $\Delta_W$, 
    denoted by $\lambda_W^{\max}$, is defined by
    \begin{equation*}
        \lambda_W^{\max} 
        \coloneq \sup_{f \in L^2(I,\nu) \setminus \{0\}} \frac{\ip{\Delta_W f}{f}_v}{\ip{f}{f}_v}.
    \end{equation*}
\end{defn}

Note that
\begin{equation*}
    \lambda_W^{\max} 
    = \sup_{f \in L^2(I,\nu) \setminus \{0\}} \frac{\norm[e]{df}^2}{\norm[v]{f}^2},
\end{equation*}
and it follows from the proof of \cite{Abhishek-Mahan24}*{Lemma 3.3} that 
$\lambda_W^{\max} \leq 4$. In fact, this bound improves to $2$, 
similar to that in the case of graphs, as shown in the following lemma.

\begin{lemma} \label{lemma:df-leq-2f}
    For any $f \in L^2(I, \nu)$, the inequality 
    $\norm[e]{df} \leq \sqrt{2} \norm[v]{f}$ holds, and consequently, we have
    $\lambda_W^{\max} \leq 2$.
\end{lemma}

\begin{proof}
    Let $f \in L^2(I, \nu)$ be arbitrary. Then we have
    \begin{equation*}
        \norm[e]{df}^2 = \int_{E} (df)^2 W
        = \int_{0}^{1} \int_{x}^{1} (f(y) - f(x))^2 W(x,y)\, \mathrm{d}y\, \mathrm{d}x.
    \end{equation*}

    Using the fact that the graphon $W$ is symmetric, it follows that
    \begin{align*}
        \int_{0}^{1} \int_{x}^{1} (f(y) - f(x))^2 W(x,y)\, \mathrm{d}y\, \mathrm{d}x
        & = 
        \int_{0}^{1} \int_{y}^{1} (f(x) - f(y))^2 W(y,x)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & =
        \int_{0}^{1} \int_{y}^{1} (f(x) - f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & = 
        \int_{E'} (df)^2 W,
    \end{align*}
    where $E' \coloneq \{(x,y) \in I^2 : y < x\}$. Now, as the function $df$ is 
    identically zero on the diagonal of $I^2$, observe that
    \begin{equation*}
        \int_{I^2} (df)^2 W = \int_{E} (df)^2 W + \int_{E'} (df)^2 W = 2 \norm[e]{df}^2.
    \end{equation*}
    Hence, we get
    \begin{align*}
        \norm[e]{df}^2 
        & = 
        \frac{1}{2} \int_0^1 \int_0^1 (f(x) - f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & \leq
        \frac{1}{2} \int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y 
        + \frac{1}{2} \int_0^1 \int_0^1 f(y)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & \quad +
        \int_0^1 \int_0^1 \abs{f(x)} \abs{f(y)} W(x,y)\, \mathrm{d}x\, \mathrm{d}y,
    \end{align*}
    that is,
    \begin{equation*}
        \norm[e]{df}^2 = \norm[v]{f}^2 
        + \int_0^1 \int_0^1 \abs{f(x)} \abs{f(y)} W(x,y)\, \mathrm{d}x\, \mathrm{d}y.
    \end{equation*}
    Now since the function $f$ lies in $L^2(I, \nu)$, the functions 
    \begin{equation*}
        (x,y) \mapsto |f(x)| \sqrt{W(x,y)} \quad \text{and} \quad 
        (x,y) \mapsto |f(y)| \sqrt{W(x,y)},
    \end{equation*}
    defined on $I^2$, are in $L^2(I^2)$. Then the Cauchy--Schwarz inequality implies that
    \begin{align*}
        & \hspace{0.56cm} \int_0^1 \int_0^1 \abs{f(x)} \abs{f(y)} W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & \leq 
        \left(\int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y\right)^\frac{1}{2} 
        \left(\int_0^1 \int_0^1 f(y)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y\right)^\frac{1}{2}
        \\
        & =
        \norm[v]{f}^2,
    \end{align*}
    from which we conclude that $\norm[e]{df}^2 \leq 2 \norm[v]{f}^2$.
\end{proof}

\section{Bipartiteness ratio of graphons} \label{section:bpratio}

Given any graph with vertex set $V$, a nonempty subset $S$ of $V$ and a bipartition
$\{L,R\}$ of $S$, Trevisan considered the ratio of the number of edges incident on $S$ 
which ``fail to be cut'' by the partition $\{L,R\}$ to the total number of edges
incident on $S$, and defined the bipartiteness ratio of the graph to be 
the minimum of such ratios over all nonempty subsets and their partitions. 
We refer to \cref{section:graphs&graphons} for the precise definition.
We extend this definition to graphons.

\begin{defn}[Bipartiteness ratio]
    The \emph{bipartiteness ratio} of a connected graphon $W$, denoted by $\beta_W$,
    is defined by
    \begin{equation*}
        \beta_W \coloneq \inf_{\substack{L,R \subseteq I \\ \text{measurable}\\
        \mu_L(L \cup R) > 0 \\ L \cap R = \emptyset}} \beta_W(L,R),
    \end{equation*}
    where for every measurable disjoint subsets $L$ and $R$ of $I$ 
    with $\mu_L(L \cup R)> 0$, we have
    \begin{equation*}
        \beta_W(L,R) 
        = \frac{2 \eta(L \times L) + 2 \eta(R \times R) + \eta((L \cup R) \times (L \cup R)^c)}
        {2 \eta((L \cup R) \times I)}.
    \end{equation*}
\end{defn}
Since the graphon $W$ is connected, the above quantity is well-defined.

We will write $\lambda_W^{\max}$, $\beta_W$ and $\beta_W(L,R)$ as $\lambda^{\max}$, 
$\beta(L,R)$ and $\beta$, respectively, when there is no room for confusion.

Khetan and Mj \cite{Abhishek-Mahan24}*{Lemma 3.2} proved that 
the Cheeger constant of any connected graphon is bounded above by $\frac{1}{2}$, 
using the strong mixing property of the doubling map. 
We follow the same arguments to prove that the bipartiteness ratio of connected graphons 
is also bounded above by $\frac{1}{2}$.

For the sake of completeness we define the notion of strong mixing and state 
its characterization that we will use here.

\begin{defn}[Strong mixing]
    Let $(\Omega,\mathcal{A},\mu)$ be a measure space. A measurable function 
    $\map{T}{\Omega}{\Omega}$ is called \emph{strong mixing} if it is a measure preserving 
    transformation, that is, $\mu(A) = \mu(T^{-1}(A))$ for all $A \in \mathcal{A}$, 
    and for all measurable subsets $A$, $B \in \mathcal{A}$, the function $T$ satisfies
    \begin{equation*}
        \lim_{n \to \infty} \mu(T^{-n}(A) \cap B) = \mu(A) \mu(B).
    \end{equation*} 
\end{defn}

It is well known that the doubling map $\map{S}{I}{I}$, defined by
\begin{equation*}
    S(x) \coloneq 
    \begin{cases}
        2x 
        & \text{if }\, 0 \leq x \leq \dfrac{1}{2},
        \\
        2x-1 
        & \text{if }\, \dfrac{1}{2} < x \leq 1,
    \end{cases}
\end{equation*}
is strong mixing. Then it follows that the function $\map{T}{I^2}{I^2}$, 
defined by $T = S \times S$, is also strong mixing. 
Here $I$ and $I^2$ are endowed with the Lebesgue measure.

\begin{lemma}[Strong mixing property]
    Let $(\Omega,\mathcal{A},\mu)$ be a measure space. Then a measure preserving 
    transformation $\map{T}{\Omega}{\Omega}$ is strong mixing if and only if 
    for all $f$, $g \in L^2(\mu)$, we have
    \begin{equation*}
        \lim_{n \to \infty} \int_\Omega (f \circ T^n)g\, \mathrm{d}\mu 
        = \int_\Omega f\, \mathrm{d}\mu \int_\Omega g\, \mathrm{d}\mu.
    \end{equation*}
\end{lemma}

We denote the characteristic function of a set $A$ by $1_A$. 

\begin{lemma} \label{lemma:beta-leq-half}
    For every connected graphon $W$, the inequality $\beta_W \leq \frac{1}{2}$ holds.
\end{lemma}

\begin{proof}
    Let $S$ denote the doubling map on $I$, as defined above, $T$ denote 
    the map $S \times S$, and $\eta_L$ denote the Lebesgue measure on $I^2$.
    Set $L = \left[0, \frac{1}{2}\right]$ and $R = \left(\frac{1}{2}, 1\right]$.
    Using the strong mixing property for $T$, we get
    \begin{equation*}
        \lim_{n \to \infty} \int_{I^2} (1_{L \times L} \circ T^n)W\, \mathrm{d}\eta_L 
        = \left(\int_{I^2} 1_{L \times L}\, \mathrm{d}\eta_L\right)
        \left(\int_{I^2} W\, \mathrm{d}\eta_L\right) = \frac{\eta(I^2)}{4},
    \end{equation*}
    \begin{equation*}
        \lim_{n \to \infty} \int_{I^2} (1_{R \times R} \circ T^n)W\, \mathrm{d}\eta_L 
        = \left(\int_{I^2} 1_{R \times R}\, \mathrm{d}\eta_L\right)
        \left(\int_{I^2} W\, \mathrm{d}\eta_L\right) = \frac{\eta(I^2)}{4},
    \end{equation*}
    \begin{equation*}
        \lim_{n \to \infty} \int_{I^2} (1_{(L \cup R) \times (L \cup R)^c} 
        \circ T^n)W\, \mathrm{d}\eta_L = 0, \tag{since the set $(L \cup R)^c$ is empty}
    \end{equation*}
    and
    \begin{equation*}
        \lim_{n \to \infty} \int_{I^2} (1_{(L \cup R) \times I} \circ T^n)W\, \mathrm{d}\eta_L 
        = \left(\int_{I^2} 1_{(L \cup R) \times I}\, \mathrm{d}\eta_L\right)
        \left(\int_{I^2} W\, \mathrm{d}\eta_L\right) = \eta(I^2).
    \end{equation*}
    For every $n \geq 1$, let $L_n$ and $R_n$ denote the measurable subsets 
    $S^{-n}(L)$ and $S^{-n}(R)$ of $I$, respectively. Since $L$ and $R$ are disjoint, 
    so are the sets $L_n$ and $R_n$ for all $n$. Also, the fact that $S$ is 
    measure preserving ensures that $\mu_L(L_n \cup R_n) > 0$. Observe that
    \begin{align*}
        & \hspace{0.56cm} \beta_W(L_n,R_n)
        \\
        & = 
        \frac{2 \eta(L_n \times L_n) + 2 \eta(R_n \times R_n) 
        + \eta((L_n \cup R_n) \times (L_n \cup R_n)^c)}{2 \eta((L_n \cup R_n) \times I)}
        \\
        & =
        \frac{2 \int_{I^2} 1_{L_n \times L_n} W\, \mathrm{d}\eta_L 
        + 2 \int_{I^2} 1_{R_n \times R_n} W\, \mathrm{d}\eta_L
        + \int_{I^2} 1_{(L_n \cup R_n) \times (L_n \cup R_n)^c} W\, \mathrm{d}\eta_L}
        {2 \int_{I^2} 1_{(L_n \cup R_n) \times I} W\, \mathrm{d}\eta_L}.
    \end{align*}
    Taking limit as $n$ tends to $\infty$ in the above, and using the fact that 
    \begin{equation*}
        1_{A \times B} \circ T^n = 1_{T^{-n}(A \times B)} = 1_{S^{-n}(A) \times S^{-n}(B)},
    \end{equation*}
    for all $n \geq 1$ and subsets $A, B$ of $I$, it follows that
    \begin{equation*}
        \lim_{n \to \infty} \beta_W(L_n,R_n)
        = \frac{\frac{\eta(I^2)}{2} + \frac{\eta(I^2)}{2}}{2 \eta(I^2)} = \frac{1}{2}.
    \end{equation*}
    Now since for all $n$, we have $\beta_W \leq \beta_W(L_n,R_n)$, it follows that
    $\beta_W \leq \frac{1}{2}$.
\end{proof}

\begin{remark}
    In fact, the bound in the above lemma is sharp as can be seen from the following example. 
    If $W$ is a nonzero constant graphon, then for any disjoint measurable subsets
    $L$ and $R$ of $I$ with $\mu_L(L \cup R) > 0$, we have
    \begin{align*}
        \beta_W(L,R) \
        & =
        \frac{2 \eta(L \times L) + 2 \eta(R \times R) + \eta((L \cup R) \times (L \cup R)^c)}
        {2 \eta((L \cup R) \times I)}
        \\
        & =
        \frac{2 \mu_L(L)^2 + 2 \mu_L(R)^2 + (\mu_L(L) + \mu_L(R)) (1 - (\mu_L(L) + \mu_L(R)))}
        {2 (\mu_L(L) + \mu_L(R))}
        \\
        & =
        \frac{1}{2} + \frac{2 \mu_L(L)^2 + 2 \mu_L(R)^2 - (\mu_L(L) + \mu_L(R))^2}
        {2 (\mu_L(L) + \mu_L(R))}
        \\
        & =
        \frac{1}{2} + \frac{(\mu_L(L) - \mu_L(R))^2}{2 (\mu_L(L) + \mu_L(R))}
        \\
        & \geq \frac{1}{2},
    \end{align*} 
    and hence, using \cref{lemma:beta-leq-half}, we conclude that 
    the bipartiteness ratio of $W$ is equal to $\frac{1}{2}$.
\end{remark}

\section[The dual Cheeger--Buser inequality for graphons]
{The dual Cheeger--Buser inequality for graphons
\sectionmark{The dual Cheeger--Buser ineq. for graphons}} \label{section:CBIneq}
\sectionmark{The dual Cheeger--Buser ineq. for graphons}

\subsection{The dual Buser inequality}

Here we establish an upper bound for $2 - \lambda^{\max}$, by obtaining a characterization 
of $\beta$ in terms of functions taking values in $\{-1,0,1\}$, 
extending Trevisan's idea to graphons.

\begin{lemma} \label{lemma:buser}
    For every connected graphon $W$, the inequality $2 - \lambda^{\max} \leq 2 \beta$ holds.
\end{lemma}

\begin{proof}
    Note that
    \begin{align*}
        & \hspace{0.55cm} 2 - \lambda^{\max} 
        \\
        & = 
        2 - \sup_{f \in L^2(I, \nu) \setminus \{0\}} \frac{\norm[e]{df}^2}{\norm[v]{f}^2}
        \\
        & =
        2 - \sup_{f \in L^2(I, \nu) \setminus \{0\}} 
        \frac{\int_0^1 \int_0^1 (f(x) - f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        {2 \int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        \\
        & =
        \inf_{f \in L^2(I, \nu) \setminus \{0\}} 
        \left(2 - \frac{\int_0^1 \int_0^1 (f(x) - f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        {2 \int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}\right)
        \\
        & =
        \inf_{f \in L^2(I, \nu) \setminus \{0\}}
        \frac{4 \int_{I^2} f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        - \int_{I^2} (f(x) - f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        {2 \int_{I^2} f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}.
    \end{align*}
    The numerator in the above expression is
    \begin{align*}
        & \hspace{0.56cm}
        4 \int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        - \int_0^1 \int_0^1 (f(x) - f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & =
        3 \int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        - \int_0^1 \int_0^1 f(y)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & \quad +
        2 \int_0^1 \int_0^1 f(x) f(y) W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & =
        2 \int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        + 2 \int_0^1 \int_0^1 f(x) f(y) W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & =
        \int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y 
        + \int_0^1 \int_0^1 f(y)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & \quad +
        2 \int_0^1 \int_0^1 f(x) f(y) W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & =
        \int_0^1 \int_0^1 (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y.
    \end{align*}
    Therefore, it follows that
    \begin{equation} \label{eq:2-lambdaMax}
        2 - \lambda^{\max} = \inf_{f \in L^2(I, \nu) \setminus \{0\}}
        \frac{\int_0^1 \int_0^1 (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        {2 \int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}.
    \end{equation}
    In order to show that $2 - \lambda^{\max} \leq 2 \beta$, we now proceed 
    to obtain an expression for $\beta$ in terms of functions defined on $I$.
    
    Given any disjoint measurable subsets $L$ and $R$ of $I$ with $\mu_L(L \cup R) > 0$,
    define a function $\map{f}{I}{\{-1,0,1\}}$ for every $x \in I$ as follows.
    \begin{equation*}
        f(x) = 
        \begin{cases}
            -1 & \text{if } x \in L,
            \\
            1 & \text{if } x \in R,
            \\
            0 & \text{if } x \notin L \cup R.
        \end{cases}
    \end{equation*}
    Then $f$ is a nonzero function in $L^2(I, \nu)$, and we have
    \begin{equation} \label{eq:partitions<->fns}
        \beta_W(L,R)
        = \frac{\int_{0}^{1} \int_{0}^{1} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        {4 \int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}.
    \end{equation}
    To see this, using the definition of the function $f$, observe that
    \begin{align*}
        & \hspace{0.49cm}
        \int_{0}^{1} \int_{0}^{1} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y 
        \\
        & = 
        \int_{L \cup R} \int_{L \cup R} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & \quad +
        \int_{L \cup R} \int_{(L \cup R)^c} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & \quad +
        \int_{(L \cup R)^c} \int_{L \cup R} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & \quad +
        \int_{(L \cup R)^c} \int_{(L \cup R)^c} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & =
        \int_{L \cup R} \int_{L \cup R} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        + 2 \eta((L \cup R) \times (L \cup R)^c)
        \\
        & = 
        \int_L \int_L (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        + \int_L \int_R (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y 
        \\
        & \quad +
        \int_R \int_L (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        + \int_R \int_R (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y 
        \\
        & \quad +
        2 \eta((L \cup R) \times (L \cup R)^c) 
        \\
        & =
        4 \eta(L \times L) + 4 \eta(R \times R) + 2 \eta((L \cup R) \times (L \cup R)^c),
    \end{align*}
    and we also have
    \begin{align*} 
        & \hspace{0.6cm} 
        \int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y 
        \\
        & =
        \int_{L \cup R} \int_I f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        + \int_{(L \cup R)^c} \int_I f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & =
        \int_{L \cup R} \int_I W(x,y)\, \mathrm{d}x\, \mathrm{d}y 
        \\
        & =
        \eta((L \cup R) \times I).
    \end{align*}
    On the other hand, given a nonzero function $\map{f}{I}{\{-1,0,1\}}$ in $L^2(I, \nu)$,
    the sets $L = f^{-1}(-1)$ and $R = f^{-1}(1)$ are disjoint measurable subsets
    of $I$ with $\mu_L(L \cup R) > 0$ such that \cref{eq:partitions<->fns} holds. 
    Hence, we conclude that
    \begin{equation} \label{eq:beta-with-fns}
        \beta_W = \inf_{\substack{\map{f}{I}{\{-1,0,1\}} \\ f \in L^2(I, \nu) \setminus \{0\}}}
        \frac{\int_{0}^{1} \int_{0}^{1} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        {4 \int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}.
    \end{equation}
    Now combine \cref{eq:2-lambdaMax} and \cref{eq:beta-with-fns} to get the inequality 
    $2 - \lambda^{\max} \leq 2 \beta$.
\end{proof}

\subsection{The dual Cheeger inequality}

We obtain a lower bound on $2 - \lambda^{\max}$ with the help of some lemmas. 
The following lemma allows us to work with just essentially bounded functions
instead of all $L^2$ functions while dealing with $\lambda^{\max}$.
It is inspired from the analogous lemma in the work of Khetan and Mj 
\cite{Abhishek-Mahan24}*{Lemma 5.4}.

\begin{lemma} \label{lemma:lambda-max-Linfty}
    Given a connected graphon $W$, we have
    \begin{equation*}
        \lambda^{\max} = \sup_{f \in L^{\infty}(I, \nu) \setminus \{0\}}
        \frac{\norm[e]{df}^2}{\norm[v]{f}^2},
    \end{equation*}
    and consequently, the equality 
    \begin{equation*}
        2 - \lambda^{\max} = \inf_{f \in L^{\infty}(I, \nu) \setminus \{0\}}
        \frac{\int_0^1 \int_0^1 (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        {2 \int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}
    \end{equation*}
    holds.
\end{lemma}

\begin{proof}
    Since the measure space $I$ is $\nu$-finite, it is clear from the inclusion
    $L^{\infty}(I, \nu) \subseteq L^2(I, \nu)$ that $\lambda^{\max}$ is an upper 
    bound of the set 
    \begin{equation*}
        \left\{\frac{\norm[e]{df}^2}{\norm[v]{f}^2} : 
        f \in L^{\infty}(I, \nu) \setminus \{0\}\right\}.
    \end{equation*}
    Let $\varepsilon > 0$ be arbitrary. It is enough to show, for the first part,
    that there is a function $g \in L^{\infty}(I, \nu) \setminus \{0\}$ such that 
    the inequality
    \begin{equation*}
        \lambda^{\max} - \varepsilon < \frac{\norm[e]{dg}^2}{\norm[v]{g}^2}
    \end{equation*} 
    holds. The definition of $\lambda^{\max}$ guarantees the existence of a function
    $f \in L^2(I, \nu) \setminus \{0\}$ with 
    \begin{equation*}
        \lambda^{\max} - \frac{\varepsilon}{2} < \frac{\norm[e]{df}^2}{\norm[v]{f}^2}.
    \end{equation*} 
    So, we are done once we find $g \in L^{\infty}(I, \nu) \setminus \{0\}$ satisfying
    \begin{equation} \label{ineq:f&g-close}
        \frac{\norm[e]{df}^2}{\norm[v]{f}^2} - \frac{\norm[e]{dg}^2}{\norm[v]{g}^2}
        \leq \frac{\varepsilon}{2}.
    \end{equation}
    If the function $df$ is zero, then \eqref{ineq:f&g-close} holds by taking $g$ 
    to be the constant function $1$. Suppose $df$ is nonzero, and define 
    $M = \min\{\norm[v]{f}, \norm[e]{df}\}$, which is a positive real number. 
    As the space $L^{\infty}(I, \nu)$ is dense in $L^2(I, \nu)$, there exists 
    a function $g \in L^{\infty}(I, \nu)$ such that $\norm[v]{g-f} < \varepsilon'M$, 
    where $\varepsilon' = \min\left\{\frac{1}{\sqrt{2}}, \frac{(\sqrt{2} - 1)^2 \varepsilon}
    {16 (\sqrt{2} + 1)}\right\}$. Then, we get the inequality
    \begin{equation} \label{ineq:f&g}
        (1 - \varepsilon')\norm[v]{f} \leq \norm[v]{f} - \varepsilon'M
        < \norm[v]{g} < \norm[v]{f} + \varepsilon'M \leq (1 + \varepsilon')\norm[v]{f}.
    \end{equation}
    This ensures that $g$ is a nonzero function. Using \cref{lemma:df-leq-2f}, since we have 
    \begin{equation*}
        \norm[e]{d(g-f)} < \sqrt{2} \varepsilon'M \leq \sqrt{2} \varepsilon' \norm[e]{df},
    \end{equation*} 
    it follows that
    \begin{equation} \label{ineq:df&dg}
        (1 - \sqrt{2} \varepsilon')\norm[e]{df} < \norm[e]{dg} 
        < (1 + \sqrt{2} \varepsilon')\norm[e]{df}.
    \end{equation}
    Using \eqref{ineq:f&g} and the fact that $\varepsilon' \leq 1$, we get
    \begin{equation} \label{ineq:den-lambda-infty}
        \norm[v]{f}^2 \norm[v]{g}^2 \geq (1 - \varepsilon')^2 \norm[v]{f}^4,
    \end{equation}
    and combining \eqref{ineq:f&g}, \eqref{ineq:df&dg} and \cref{lemma:df-leq-2f} 
    gives us that
    \begin{equation} \label{ineq:num-lambda-infty}
        \norm[e]{df}^2 \norm[v]{g}^2 - \norm[e]{dg}^2 \norm[v]{f}^2 \leq ((1 + \varepsilon')^2 
        - (1 - \sqrt{2} \varepsilon')^2) \norm[v]{f}^2 \norm[e]{df}^2.
    \end{equation} 
    Now using \eqref{ineq:den-lambda-infty}, \eqref{ineq:num-lambda-infty} and the fact 
    that $\norm[e]{df}^2 \leq 2\norm[v]{f}^2$ along with the definition of $\varepsilon'$, 
    we arrive at the desired inequality \eqref{ineq:f&g-close}. 

    In order to prove the second part, repeat the arguments used to obtain 
    \cref{eq:2-lambdaMax} by replacing $L^2(I, \nu)$ with $L^{\infty}(I, \nu)$.
\end{proof}

In the following lemma, we estimate the integrals of certain ``suitable'' functions 
so that those estimates combined with \cref{lemma:f/g&intf/intg} give an upper bound 
for $\beta$ in terms of $\lambda^{\max}$. This follows ideas in Trevisan's proof 
(see \cite{Trevisan-MaxCut12}*{Section 3.2} and \cite{Trevisan-notes-expanders}*{Chapter 6}) 
of the dual Cheeger inequality for graphs. 

\begin{lemma} \label{lemma:main}
    Let $f$ be an arbitrary element of $L^2(I, \nu)$. For every $t > 0$, let $L_t$
    and $R_t$ denote the sets $f^{-1}((-\infty, -t])$ and $f^{-1}([t, \infty))$,
    respectively. Then the following inequalities hold.
    \begin{align} \label{ineq:int-num}
        & \hspace{0.6cm}
        \int_{0}^{\infty} 2t \left[2 \eta(L_t \times L_t) + 2 \eta(R_t \times R_t)
        + \eta((L_t \cup R_t) \times (L_t \cup R_t)^c)\right] \mathrm{d}t \nonumber
        \\
        & \leq
        2 \left(\int_{I^2} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \right)^\frac{1}{2} \left(\int_{I^2} f(x)^2 W(x,y)\, 
        \mathrm{d}x\, \mathrm{d}y\right)^\frac{1}{2},
    \end{align}
    and
    \begin{equation} \label{eq:int-den}
        \int_{0}^{\infty} 2t \left[2 \eta((L_t \cup R_t) \times I)\right] \mathrm{d}t
        = 2 \int_{0}^{1} \int_{0}^{1} f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y.
    \end{equation}
\end{lemma}

\begin{proof}
    Using the Fubini--Tonelli theorem, note that
    \begin{align*}
        & \hspace{0.51cm}
        \int_{0}^{\infty} 2t \left[2 \eta(L_t \times L_t) + 2 \eta(R_t \times R_t)
        + \eta((L_t \cup R_t) \times (L_t \cup R_t)^c)\right] \mathrm{d}t 
        \\
        & = 
        \int_{0}^{\infty} 2t \left[\int_{I^2} \left(2 \cdot 1_{L_t \times L_t}
        + 2 \cdot 1_{R_t \times R_t} + 1_{(L_t \cup R_t) \times (L_t \cup R_t)^c}\right) 
        W(x,y)\, \mathrm{d}x\, \mathrm{d}y\right] \mathrm{d}t
        \\
        & = 
        \int_{I^2} \left(\int_{0}^{\infty} 2t 
        (2 \cdot 1_{L_t \times L_t}(x,y) + 2 \cdot 1_{R_t \times R_t}(x,y) 
        1_{(L_t \cup R_t) \times (L_t \cup R_t)^c}(x,y))\, \mathrm{d}t\right)
        \\
        & \hspace{11cm}
        W(x,y)\, \mathrm{d}x\, \mathrm{d}y. 
    \end{align*}
    Define the sets
    \begin{align*}
        A_1 & = \{(x,y) \in I^2 : 0 \leq f(x) \leq f(y)\},
        \\
        A_2 & = \{(x,y) \in I^2 : 0 \leq f(y) < f(x)\},
        \\
        A_3 & = \{(x,y) \in I^2 : f(x) < f(y) \leq 0\},
        \\
        A_4 & = \{(x,y) \in I^2 : f(y) \leq f(x) \leq 0\},
        \\
        A_5 & = \{(x,y) \in I^2 : f(x)f(y) < 0, \abs{f(y)} < \abs{f(x)}\}.
    \end{align*}
    Now given any $(x,y) \in I^2$, observe that
    \begin{equation*}
        1_{L_t \times L_t}(x,y) =
        \begin{cases}
            1 & \text{if } (x,y) \in A_3 \text{ and } t \in (0, -f(y)],
            \\
            1 & \text{if } (x,y) \in A_4 \text{ and } t \in (0, -f(x)],
            \\
            0 & \text{otherwise,}
        \end{cases} 
    \end{equation*}
    and
    \begin{equation*}
        1_{R_t \times R_t}(x,y) =
        \begin{cases}
            1 & \text{if } (x,y) \in A_1 \text{ and } t \in (0, f(x)],
            \\
            1 & \text{if } (x,y) \in A_2 \text{ and } t \in (0, f(y)],
            \\
            0 & \text{otherwise,}
        \end{cases} 
    \end{equation*}
    and that
    \begin{equation*}
        1_{(L_t \cup R_t) \times (L_t \cup R_t)^c}(x,y) =
        \begin{cases}
            1 & \text{if } (x,y) \in A_2 \text{ and } t \in (f(y), f(x)],
            \\
            1 & \text{if } (x,y) \in A_3 \text{ and } t \in (-f(y), -f(x)],
            \\
            1 & \text{if } (x,y) \in A_5 \text{ and } t \in (\abs{f(y)}, \abs{f(x)}],
            \\
            0 & \text{otherwise.}
        \end{cases} 
    \end{equation*}
    Thus, we get
    \begin{align*}
        & \hspace{0.6cm} \int_0^1 \int_{0}^{1} 
        \left[\int_{0}^{\infty} 2t \left(2 \cdot 1_{L_t \times L_t}(x,y)
        \right) \mathrm{d}t\right] W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & = 
        \int_{A_3} \left(\int_{0}^{-f(y)} 4t\, \mathrm{d}t\right) W(x,y)\, 
        \mathrm{d}x\, \mathrm{d}y + \int_{A_4} \left(\int_{0}^{-f(x)} 4t\, 
        \mathrm{d}t\right) W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & =
        \int_{A_3} 2f(y)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y 
        + \int_{A_4} 2f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y,
    \end{align*}
    and
    \begin{align*}
        & \hspace{0.6cm} \int_0^1 \int_{0}^{1} 
        \left[\int_{0}^{\infty} 2t \left(2 \cdot 1_{R_t \times R_t}(x,y)
        \right) \mathrm{d}t\right] W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & = 
        \int_{A_1} \left(\int_{0}^{f(x)} 4t\, \mathrm{d}t\right) W(x,y)\, 
        \mathrm{d}x\, \mathrm{d}y + \int_{A_2} \left(\int_{0}^{f(y)} 4t\, 
        \mathrm{d}t\right) W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & =
        \int_{A_1} 2f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y 
        + \int_{A_2} 2f(y)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y,
    \end{align*}
    and 
    \begin{align*}
        & \hspace{0.6cm} \int_0^1 \int_{0}^{1} 
        \left[\int_{0}^{\infty} 2t \left(1_{(L_t \cup R_t) \times (L_t \cup R_t)^c}
        (x,y)\right) \mathrm{d}t\right] W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & = 
        \int_{A_2} \left(\int_{f(y)}^{f(x)} 2t\, \mathrm{d}t\right) W(x,y)\, 
        \mathrm{d}x\, \mathrm{d}y + \int_{A_3} \left(\int_{-f(y)}^{-f(x)} 2t\, 
        \mathrm{d}t\right) W(x,y)\, \mathrm{d}x\, \mathrm{d}y 
        \\
        & \quad +
        \int_{A_5} \left(\int_{\abs{f(y)}}^{\abs{f(x)}} 2t\, \mathrm{d}t\right)
        W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & =
        \int_{A_2} (f(x)^2 - f(y)^2) W(x,y)\, \mathrm{d}x\, \mathrm{d}y 
        + \int_{A_3} (f(x)^2 - f(y)^2) W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & \quad +
        \int_{A_5} (f(x)^2 - f(y)^2) W(x,y)\, \mathrm{d}x\, \mathrm{d}y.
    \end{align*}
    Therefore, we finally have
    \begin{align*}
        & \hspace{0.6cm}
        \int_{0}^{\infty} 2t \left[2 \eta(L_t \times L_t) + 2 \eta(R_t \times R_t)
        + \eta((L_t \cup R_t) \times (L_t \cup R_t)^c)\right] \mathrm{d}t 
        \\
        & =
        \int_{A_1} 2f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y 
        + \int_{A_2} (f(x)^2 + f(y)^2) W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & \quad +
        \int_{A_3} (f(x)^2 + f(y)^2) W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        + \int_{A_4} 2f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & \quad +
        \int_{A_5} (f(x)^2 - f(y)^2) W(x,y)\, \mathrm{d}x\, \mathrm{d}y,
    \end{align*}
    which is finite, as the function $f$ lies in $L^2(I, \nu)$.  
    Now since each of the above integrands is less than or equal to $\abs{f(x) + f(y)}
    (\abs{f(x)} + \abs{f(y)}) W(x,y)$, it follows that
    \begin{align*}
        & \hspace{0.51cm}
        \int_{0}^{\infty} 2t \left[2 \eta(L_t \times L_t) + 2 \eta(R_t \times R_t)
        + \eta((L_t \cup R_t) \times (L_t \cup R_t)^c)\right] \mathrm{d}t 
        \\
        & \leq
        \int_{0}^{1} \int_{0}^{1} \abs{f(x) + f(y)}(\abs{f(x)} + \abs{f(y)})
        W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & \leq 
        \left(\int_{0}^{1} \int_{0}^{1} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, 
        \mathrm{d}y\right)^\frac{1}{2} 
        \\
        & \quad
        \left(\int_{0}^{1} \int_{0}^{1} (\abs{f(x)} + \abs{f(y)})^2
        W(x,y)\, \mathrm{d}x\, \mathrm{d}y\right)^\frac{1}{2}
        \tag{using the Cauchy--Schwarz inequality in $L^2(I^2)$}
        \\
        & \leq 
        \left(\int_{0}^{1} \int_{0}^{1} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, 
        \mathrm{d}y\right)^\frac{1}{2} 
        \\
        & \quad
        \left(\int_{0}^{1} \int_{0}^{1} (2f(x)^2 + 2f(y)^2)
        W(x,y)\, \mathrm{d}x\, \mathrm{d}y\right)^\frac{1}{2}
        \tag{for real numbers $a, b$, $(\abs{a} + \abs{b})^2 \leq 2a^2 + 2b^2$}
        \\
        & =
        2 \left(\int_{I^2} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \right)^\frac{1}{2} \left(\int_{I^2} f(x)^2 W(x,y)\, 
        \mathrm{d}x\, \mathrm{d}y\right)^\frac{1}{2}.
    \end{align*}
    Similar calculations give us
    \begin{align*}
        & \hspace{0.51cm}
        \int_{0}^{\infty} 2t \left(2 \eta((L_t \cup R_t) \times I)\right) \mathrm{d}t
        \\
        & =
        \int_{0}^{1} \int_{0}^{1} \left(\int_{0}^{\infty} 4t \cdot 1_{(L_t \cup R_t)
        \times I}(x,y)\, \mathrm{d}t\right) W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & =
        \int_{0}^{1} \int_{0}^{1} \left(\int_{0}^{\abs{f(x)}} 4t\, \mathrm{d}t\right) 
        W(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & =
        2 \int_{0}^{1} \int_{0}^{1} f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y,
    \end{align*}
    as desired.
\end{proof}

Note that the measures $\mu_L$ and $\nu$ on $I$ are absolutely continuous with
respect to each other, and hence we have $L^{\infty}(I, \mu_L) = L^{\infty}(I, \nu)$.
Henceforth, we will denote these spaces by $L^{\infty}(I)$. Also, observe that 
if $f$ lies in $L^{\infty}(I)$, then its essential suprema with respect to 
both the measures are the same. We denote them by $\norm[\infty]{f}$.

\begin{proof}[Proof of \cref{thm:main}]
    We have already proved one of the inequalities in \cref{lemma:buser}. 
    For the other inequality, thanks to \cref{lemma:lambda-max-Linfty}, it suffices 
    to show that for every nonzero function $f \in L^{\infty}(I)$, there exist 
    disjoint measurable subsets $L$ and $R$ of $I$ with $\mu_L(L \cup R) > 0$ such that 
    the following inequality holds.
    \begin{equation*}
        \beta(L,R) 
        \leq \left(\frac{\int_0^1 \int_0^1 (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        {\int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}\right)^\frac{1}{2}.
    \end{equation*}

    Let $f$ be any nonzero function in $L^{\infty}(I)$. 
    For every $t \in (0, \norm[\infty]{f})$, the sets $L_t$ and $R_t$, as defined
    in \cref{lemma:main}, are disjoint measurable subsets of $I$ with 
    $\mu_L(L_t \cup R_t) > 0$. Also, for $t > \norm[\infty]{f}$, the sets 
    $L_t$ and $R_t$ have measure zero. This implies that
    \begin{align*}
        & \hspace{0.6cm}
        \int_{0}^{\infty} 2t \left[2 \eta(L_t \times L_t) + 2 \eta(R_t \times R_t)
        + \eta((L_t \cup R_t) \times (L_t \cup R_t)^c)\right] \mathrm{d}t
        \\
        & = 
        \int_{0}^{\norm[\infty]{f}} 2t \left[2 \eta(L_t \times L_t) + 2 \eta(R_t \times R_t)
        + \eta((L_t \cup R_t) \times (L_t \cup R_t)^c)\right] \mathrm{d}t
    \end{align*}
    and that 
    \begin{equation*}
        \int_{0}^{\infty} 2t \left[2 \eta((L_t \cup R_t) \times I)\right] \mathrm{d}t
        = \int_{0}^{\norm[\infty]{f}} 2t \left[2 \eta((L_t \cup R_t) \times I)\right] \mathrm{d}t,
    \end{equation*}
    where the integrand $4t \eta((L_t \cup R_t) \times I)$ is positive for 
    every $t \in (0, \norm[\infty]{f})$. Hence, using \cref{lemma:main}, we arrive
    at the inequality
    \begin{align*}
        & \hspace{0.6cm} \frac{\int_{0}^{\norm[\infty]{f}} 2t \left[2 \eta(L_t \times L_t)
        + 2 \eta(R_t \times R_t) + \eta((L_t \cup R_t) \times (L_t \cup R_t)^c)\right] 
        \mathrm{d}t}{\int_{0}^{\norm[\infty]{f}} 2t \left[2 \eta((L_t \cup R_t) 
        \times I)\right] \mathrm{d}t}
        \\
        & \leq
        \frac{2 \left(\int_{0}^{1} \int_{0}^{1} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, 
        \mathrm{d}y\right)^\frac{1}{2} \left(\int_{0}^{1} \int_{0}^{1} f(x)^2 W(x,y)\, 
        \mathrm{d}x\, \mathrm{d}y\right)^\frac{1}{2}}
        {2 \int_{0}^{1} \int_{0}^{1} f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        \\
        & =
        \left(\frac{\int_0^1 \int_0^1 (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        {\int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}\right)^\frac{1}{2}.
    \end{align*}
    Now \cref{lemma:f/g&intf/intg} guarantees that there is a 
    $t_0 \in (0, \norm[\infty]{f})$ such that 
    \begin{equation*}
        \beta(L_{t_0},R_{t_0}) \leq 
        \left(\frac{\int_0^1 \int_0^1 (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        {\int_0^1 \int_0^1 f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}\right)^\frac{1}{2},
    \end{equation*}
    which completes the proof.
\end{proof}

\section{Bipartite graphons} \label{section:bipartiteGraphons}

Khetan and Mj \cite{Abhishek-Mahan24}*{Section 7.3} gave necessary and sufficient
conditions for a graphon to be conncted, under some suitable hypothesis.
In this section, we characterize bipartite graphons in terms of the top of the spectrum
of their Laplacians and their bipartiteness ratios, under the same hypothesis. 
We start by recalling the definition of bipartite graphons.

\begin{defn}[Bipartite graphon]
    A graphon $W$ is said to be \emph{bipartite} if there exist disjoint measurable subsets
    $L$ and $R$ of $I$ such that $L \cup R = I$ and $W$ is zero almost everywhere
    on $L \times L$ and $R \times R$ (with respect to the Lebesgue measure on $I^2$).
\end{defn}

We will use \cite{Abhishek-Mahan24}*{Lemma 7.11} which states that if $W$ is
a connected graphon and the function $d_W$ is bounded below by a positive real number,
then the operator $\map{\frac{1}{d_W} T_W}{L^2(I,\nu)}{L^2(I,\nu)}$ is compact.

\begin{lemma} \label{lemma:actualEigenVal}
    Let $W$ be a connected graphon such that $d_W$ is bounded below by a positive real number.
    Then $\lambda_W^{\max}$ is an eigenvalue of the Laplacian $\Delta_W$ of $W$.
\end{lemma}

\begin{proof}
    Since the Laplacian of $W$ is a self-adjoint bounded linear operator on the Hilbert space 
    $L^2(I,\nu)$, its top of the spectrum $\lambda_W^{\max}$ is its approximate eigenvalue,
    using \cite{BVLimaye-FA96}*{Theorem 27.5(a)}. Then $1 - \lambda_W^{\max}$ 
    is an approximate eigenvalue of the operator 
    $I - \Delta_W = I - \left(I - \frac{1}{d_W} T_W\right) 
    = \frac{1}{d_W} T_W$. Note that $\frac{1}{d_W} T_W$ is a compact operator 
    by \cite{Abhishek-Mahan24}*{Lemma 7.11}. We know that every nonzero approximate eigenvalue 
    of a compact operator on a Hilbert space is its eigenvalue 
    (see \cite{BVLimaye-FA96}*{Lemma 28.4(a)} for instance). 
    Hence, in our case, $1 - \lambda_W^{\max}$ is an eigenvalue of $\frac{1}{d_W} T_W$.
    Then it follows that $\lambda_W^{\max}$ is an eigenvalue of $\Delta_W$.
\end{proof}

The following lemma characterizes bipartite graphons.

\begin{lemma} \label{lemma:bipartiteGraphonChar}
    Let $W$ be a connected graphon such that $d_W$ is bounded below by a positive real number.
    Then the following statements are equivalent.
    \begin{enumerate}
        \item $\beta_W = 0$.
        \item $\lambda_W^{\max} = 2$.
        \item The graphon $W$ is bipartite.
    \end{enumerate}
\end{lemma}

\begin{proof}
    The implication $(1) \implies (2)$ follows from the dual Buser inequality 
    (\cref{lemma:buser}), and the implication $(3) \implies (1)$ is a direct
    consequence of the definitions of $\beta_W$ and bipartite graphons.
    Now we prove that $(2) \implies (3)$.

    Suppose that $\lambda_W^{\max} = 2$. Then \cref{lemma:actualEigenVal} ensures that 
    $2$ is an eigenvalue of $\Delta_W$. Let $f \in L^2(I,\nu)$ be 
    its corresponding eigenfunction. Then the arguments similar to those used
    to prove \cref{eq:2-lambdaMax} yield the equation
    \begin{equation*}
        \frac{\int_{I^2} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        {2 \int_{I^2} f(x)^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y} = 0,
    \end{equation*}
    and hence, 
    \begin{equation} \label{eq:num2-lambda}
        \int_{I^2} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y = 0.
    \end{equation}
    Denote the sets $f^{-1}(-\infty,0)$ and $f^{-1}(0,\infty)$ by $L$ and $R$, respectively.
    Then \cref{eq:num2-lambda} gives that 
    \begin{equation*}
        \int_{(L \cup R) \times (L \cup R)^c} 
        (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y = 0.
    \end{equation*}
    For any $(x,y) \in (L \cup R) \times (L \cup R)^c$, since we have 
    $(f(x) + f(y))^2 = f(x)^2 > 0$, it follows that $W$ is zero almost everywhere
    on $(L \cup R) \times (L \cup R)^c$. Now since $W$ is connected, this implies that
    the Lebesgue measure of either $L \cup R$ or its complement is zero. But the fact 
    that the function $f$ is nonzero forces $(L \cup R)^c$ to have measure zero.
    Let $L'$ denote the set $L \cup (L \cup R)^c$. Note that $L'$ and $R$ are
    disjoint measurable subsets of $I$, and their union is $I$. We are done
    once we show that $W$ is zero almost everywhere on $L' \times L'$ and $R \times R$.
    It follows from \cref{eq:num2-lambda} that 
    \begin{equation*}
        \int_{L \times L} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y = 0,
    \end{equation*}
    and
    \begin{equation*}    
        \int_{R \times R} (f(x) + f(y))^2 W(x,y)\, \mathrm{d}x\, \mathrm{d}y = 0.
    \end{equation*}
    For all $(x,y) \in L \times L$, the quantity $(f(x) + f(y))^2$ is positive, 
    and therefore, $W$ is zero almost everywhere on $L \times L$, and hence 
    also on $L' \times L'$, as $(L \cup R)^c$ has measure zero. 
    Similarly, it follows that $W$ is zero almost everywhere on $R \times R$.
\end{proof}

\section{Graphs and the associated graphons} \label{section:graphs&graphons}

Let $V$ denote the set $\{1, \dots, n\}$ with $n \geq 2$, and $\map{w}{V \times V}
{I}$ be a symmetric function, that is, $w(i,j) = w(j,i)$ for all $i,j \in V$.
The pair $G = (V,w)$ is called a \emph{weighted graph}. We will denote $w(i,j)$ 
by $w_{ij}$, for all $i,j \in V$. The weighted graph $G$ is said to be \emph{loopless}
if $w_{ii} = 0$ for all $i \in V$. For any subsets $A, B$ of $V$, define
\begin{equation*}
    e_G(A,B) = \sum_{i \in A, j \in B} w_{ij}.
\end{equation*}
In the following, we always assume that $G$ is \emph{connected}, which means that
for any nonempty proper subset $A$ of $V$, $e_G(A,A^c)$ is positive. 
For any $i \in V$, the \emph{volume} of $i$ is defined by $\vol(i) = \sum_{j \in V} w_{ij}$. 
Note that $\vol(i)$ is positive for all $i$, since the graph $G$ is connected.

Let $\ell^2(V)$ denote the Hilbert space 
of all functions from $V$ to $\mathbb{R}$, equipped with the inner product
\begin{equation*}
    \ip{f_1}{f_2} \coloneq \sum_{i \in V} f_1(i) f_2(i),
\end{equation*}
for every $f_1,f_2 \in \ell^2(V)$. 
The \emph{Laplacian} $\map{\Delta_G}{\ell^2(V)}{\ell^2(V)}$ of the weighted graph
$G$ is a linear operator defined by
\begin{equation*}
    (\Delta_G g)(i) \coloneq 
    g(i) - \frac{1}{\sqrt{\vol(i)}} \sum_{j \in V} \frac{g(j) w_{ij}}{\sqrt{\vol(j)}},
\end{equation*}
for every $g \in \ell^2(V)$ and $i \in V$. It is a self-adjoint operator, since 
the function $w$ is symmetric. Then the largest eigenvalue $\lambda_G^{\max}$ 
of the Laplacian $\Delta_G$ is given by
\begin{equation*}
    \lambda_G^{\max} 
    = \sup_{g \in \ell^2(V) \setminus \{0\}} \frac{\ip{\Delta_G g}{g}}{\ip{g}{g}}
    = \sup_{g \in \ell^2(V) \setminus \{0\}} 
    \frac{\ip{\Delta_G (\sqrt{D} g)}{\sqrt{D} g}}{\ip{\sqrt{D} g}{\sqrt{D} g}},
\end{equation*}
where $\sqrt{D}$ is an invertible operator on $\ell^2(V)$ defined by
\begin{equation*}
    (\sqrt{D} h)(i) = \sqrt{\vol(i)} h(i),
\end{equation*} 
for all $h \in \ell^2(V)$ and $i \in V$.
It is easy to check that
\begin{equation*}
    \lambda_G^{\max} 
    = \sup_{g \in \ell^2(V) \setminus \{0\}} 
    \frac{\sum_{i,j \in V} (g(i) - g(j))^2 w_{ij}}{2 \sum_{i,j \in V} g(i)^2 w_{ij}}.
\end{equation*}

The \emph{bipartiteness ratio} $\beta_G$ of the weighted graph $G$ is defined as follows.
\begin{equation*}
    \beta_G = \min_{\substack{A,B \subseteq V \\ A \cup B \neq \emptyset \\
    A \cap B = \emptyset}} \frac{2 e_G(A,A) + 2 e_G(B,B) + e_G(A \cup B, (A \cup B)^c)}
    {2 e_G(A \cup B,V)}.
\end{equation*}

Now given a weighted graph $G = (V, w)$, it can be viewed as the graphon, 
called the \emph{associated graphon} $W_G$ of $G$, defined as below. 
For each $1 \leq i < n$, denote the interval $[\frac{i-1}{n}, \frac{i}{n})$ by $P_i$, 
and $[\frac{n-1}{n}, 1]$ by $P_n$. Note that $\{P_i \times P_j : 1 \leq i,j \leq n\}$ 
forms a partition of $I^2$. For any $1 \leq i,j \leq n$ and $(x,y) \in P_i \times P_j$, 
define $W_G(x,y) \coloneq w_{ij}$.

We will show that the connectedness of $G$ implies the connectedness of $W_G$,
so that we can talk about the Laplacian and the bipartiteness ratio of $W_G$.

\begin{lemma}
    If $G = (V, w)$ is a connected weighted graph, then the associated graphon $W_G$ 
    of $G$ is also connected.
\end{lemma}

\begin{proof}
    Let $A$ be a measurable subset of $I$ with $0 < \mu_L(A) < 1$. Then the sets
    \begin{equation*}
        S_1 = \{i \in V : \mu_L(A \cap P_i) > 0\} \quad \text{and} \quad 
        S_2 = \{j \in V : \mu_L(A^c \cap P_j) > 0\}
    \end{equation*}
    are nonempty, and the inclusions $S_1^c \subseteq S_2$ and $S_2^c \subseteq S_1$
    hold. Further, observe that
    \begin{align*}
        \int_{A \times A^c} W 
        = \sum_{i,j \in V} \int_{(A \cap P_i) \times (A^c \cap P_j)} W 
        & = 
        \sum_{i \in S_1, j \in S_2} \int_{(A \cap P_i) \times (A^c \cap P_j)} w_{ij}
        \\
        & = 
        \sum_{i \in S_1, j \in S_2} \mu_L(A \cap P_i) \mu_L(A^c \cap P_j) w_{ij}
        \\
        & \geq
        m_A e_G(S_1,S_2),
    \end{align*} 
    where $m_A = \min\{\mu_L(A \cap P_i) \mu_L(A^c \cap P_j) : i \in S_1, j \in S_2\} > 0$.
    Now if both $S_1$ and $S_2$ are equal to $V$, then we have $e_G(S_1,S_2) \geq 
    e_G(\{1\},\{1\}^c) > 0$ as the graph $G$ is connected. Otherwise,
    if $S_1$ or $S_2$ is not $V$, then we get $e_G(S_1,S_2) \geq e_G(S_1,S_1^c) > 0$
    or $e_G(S_1,S_2) \geq e_G(S_2^c,S_2) > 0$, respectively. Thus, in any case,
    $m_A e_G(S_1,S_2)$ and hence $\int_{A \times A^c} W$ is positive, showing that
    the associated graphon $W_G$ is connected. 
\end{proof}

\subsection{Top of the spectrum of graphs and the associated graphons}

The arguments in the following lemma are similar to that in 
\cite{Abhishek-Mahan24}*{Section 4.2}.

\begin{lemma} \label{lemma:compare-lambdaG&WG}
    Given any loopless, connected weighted graph $G = (V,w)$, we have
    $\lambda_{W_G}^{\max} = \lambda_G^{\max}$.
\end{lemma}

\begin{proof}
    Let $\map{g}{V}{\mathbb{R}}$ be any nonzero function. It gives rise to 
    a nonzero function $g' \in L^{\infty}(I)$, defined for any $x \in P_i$
    with $1 \leq i \leq n$, by $g'(x) = g(i)$, that satisfies
    \begin{align*}
        \lambda_{W_G}^{\max}
        & \geq 
        \frac{\int_{0}^{1} \int_{0}^{1} (g'(x) - g'(y))^2 W_G(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        {2 \int_{0}^{1} \int_{0}^{1} g'(x)^2 W_G(x,y)\,\mathrm{d}x\, \mathrm{d}y}
        \\
        & =
        \frac{\sum_{i,j \in V} \int_{P_i \times P_j} (g'(x) - g'(y))^2 W_G(x,y)\, 
        \mathrm{d}x\, \mathrm{d}y}{2 \sum_{i,j \in V} \int_{P_i \times P_j}
        g'(x)^2 W_G(x,y)\,\mathrm{d}x\, \mathrm{d}y}
        \\
        & = 
        \frac{\sum_{i,j \in V} (g(i) - g(j))^2 w_{ij}}{2 \sum_{i,j \in V} g(i)^2 w_{ij}}.
    \end{align*}  
    Hence, we get the inequality $\lambda_{W_G}^{\max} \geq \lambda_G^{\max}$.
    
    On the other hand, given a nonzero function $f \in L^{\infty}(I)$, define 
    the function $\map{F}{V}{\mathbb{R}}$ by $F(i) = \int_{P_i} f(x)\, \mathrm{d}x$,
    for every $i \in V$.
    Then the definition of $\lambda_G^{\max}$ gives the inequality
    \begin{equation*}
        \frac{1}{2} \sum_{i,j \in V} (F(i) - F(j))^2 w_{ij} 
        \leq \lambda_G^{\max} \sum_{i,j \in V} F(i)^2 w_{ij},
    \end{equation*}
    that is,
    \begin{equation*}
        \sum_{i,j \in V} F(i)^2 w_{ij} - \sum_{i,j \in V} F(i) F(j) w_{ij}
        \leq \lambda_G^{\max} \sum_{i,j \in V} F(i)^2 w_{ij},
    \end{equation*}
    and hence, we have
    \begin{equation} \label{ineq:largestEVforG}
        - \sum_{i,j \in V} F(i) F(j) w_{ij} 
        \leq (\lambda_G^{\max} - 1) \sum_{i,j \in V} F(i)^2 w_{ij}.
    \end{equation}
    Now note that
    \begin{align*}
        \sum_{i,j \in V} F(i) F(j) w_{ij} 
        & =
        \sum_{i,j \in V} \left(\int_{P_i} f(x)\, \mathrm{d}x\right) 
        \left(\int_{P_j} f(y)\, \mathrm{d}y\right) w_{ij}
        \\
        & =
        \sum_{i,j \in V} \int_{P_i \times P_j} f(x) f(y) W_G(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & =
        \int_{0}^{1} \int_{0}^{1} f(x) f(y) W_G(x,y)\, \mathrm{d}x\, \mathrm{d}y,
    \end{align*}
    and that
    \begin{align*}
        \sum_{i,j \in V} F(i)^2 w_{ij}
        & =
        \sum_{i,j \in V} \left(\int_{P_i} f(x)\, \mathrm{d}x\right)^2 w_{ij}
        \\
        & \leq
        \frac{1}{n} \sum_{i,j \in V} \left(\int_{P_i} f(x)^2\, \mathrm{d}x\right) w_{ij}
        \tag{using the Cauchy--Schwarz inequality}
        \\
        & =
        \sum_{i,j \in V} \left(\int_{P_i} f(x)^2\, \mathrm{d}x\right) 
        \left(\int_{P_j} 1\, \mathrm{d}y\right) w_{ij}
        \\
        & =
        \sum_{i,j \in V} \int_{P_i \times P_j} f(x)^2 W_G(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \\
        & =
        \int_{0}^{1} \int_{0}^{1} f(x)^2 W_G(x,y)\, \mathrm{d}x\, \mathrm{d}y.
    \end{align*}
    Thus, using the fact that the largest eigenvalue of the Laplacian of  a loopless graph
    is $\geq 1$ \cite{Chung-SpectralGraphTh97}*{Lemma 1.7(ii)}, 
    \eqref{ineq:largestEVforG} becomes
    \begin{equation*}
        - \int_{0}^{1} \int_{0}^{1} f(x) f(y) W_G(x,y)\, \mathrm{d}x\, \mathrm{d}y
        \leq (\lambda_G^{\max} - 1) 
        \int_{0}^{1} \int_{0}^{1} f(x)^2 W_G(x,y)\, \mathrm{d}x\, \mathrm{d}y,
    \end{equation*}
    which implies
    \begin{equation*}
        \frac{\norm[e]{df}^2}{\norm[v]{f}^2}
        = 1 - \frac{\int_{0}^{1} \int_{0}^{1} f(x) f(y) W_G(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        {\int_{0}^{1} \int_{0}^{1} f(x)^2 W_G(x,y)\, \mathrm{d}x\, \mathrm{d}y}
        \leq 1 + (\lambda_G^{\max} - 1)  = \lambda_G^{\max}.
    \end{equation*}
    This proves that $\lambda_{W_G}^{\max} \leq \lambda_G^{\max}$, as desired.
\end{proof}

\begin{remark}
    Combining \cref{lemma:bipartiteGraphonChar} and \cref{lemma:compare-lambdaG&WG} 
    with the fact that a connected graph is bipartite if and only if the largest eigenvalue
    of its Laplacian is $2$, we conclude that a connected graph is bipartite
    if and only if its associated graphon is bipartite.
\end{remark}

\subsection{Bipartiteness ratio of graphs and the associated graphons}

Let $G = (V,w)$ be a connected weighted graph. Recall that $V = \{1, \dots, n\}$
with $n \geq 2$. We now obtain a characterization for the bipartite ratio $\beta_{W_G}$
of the associated graphon $W_G$ of $G$ in terms of certain elements of $I^n$, 
analogous to the notion of the fractional Cheeger constant introduced by 
Khetan and Mj \cite{Abhishek-Mahan24}.

For every $\alpha = (\alpha_1, \dots, \alpha_n), \gamma = (\gamma_1, \dots, \gamma_n)
\in I^n$ with $0 < \alpha + \gamma \leq 1$, where $0 = (0, \dots, 0), 
1 = (1, \dots, 1) \in I^n$, define
\begin{equation*}
    \tilde \beta_G(\alpha,\gamma) \coloneq 
    \frac{\sum_{i,j \in V} [2 \alpha_i \alpha_j + 2 \gamma_i \gamma_j 
    + (\alpha_i + \gamma_i)(1 - (\alpha_j + \gamma_j))] w_{ij}}
    {2 \sum_{i,j \in V} (\alpha_i + \gamma_i) w_{ij}}.
\end{equation*}
Note that $\sum_{i,j \in V} (\alpha_i + \gamma_i) w_{ij} 
= \sum_{i,j \in V} (\alpha_i + \gamma_i) \vol(i)$, which is positive, 
since $\vol(i) > 0$ for all $i$.

\begin{lemma} \label{lemma:beta-WG-tilde}
    Given a connected weighted graph  $G = (V,w)$, we have
    \begin{equation} \label{eq:beta-WG-tilde}
        \beta_{W_G} = \inf_{\substack{\alpha, \gamma \in I^n \\ 0 < \alpha + \gamma \leq 1}} 
        \tilde \beta_G(\alpha,\gamma).
    \end{equation}
\end{lemma}

\begin{proof}
    It suffices to prove that the sets
    \begin{equation*}
        A = \{\tilde \beta_G(\alpha,\gamma) : \alpha, \gamma \in I^n, 
        0 < \alpha + \gamma \leq 1\},
    \end{equation*}
    and 
    \begin{equation*}
        B = \{\beta_{W_G}(L,R) : L, R \text{ are disjoint measurable subsets of } I,  
        \mu_L(L \cup R) > 0\}
    \end{equation*}
    are equal. Let $\alpha = (\alpha_1, \dots, \alpha_n), 
    \gamma = (\gamma_1, \dots, \gamma_n)$ be elements of $I^n$ with 
    $0 < \alpha + \gamma \leq 1$. Then, observe that the sets 
    \begin{equation*}
        L = \bigcup_{i \in V} \left(\frac{i-1}{n}, \frac{i - 1 + \alpha_i}{n}\right) 
        \quad \text{and} \quad 
        R = \bigcup_{j \in V} \left(\frac{j - \gamma_j}{n}, \frac{j}{n}\right)
    \end{equation*} 
    are disjoint measurable subsets of $I$ and $\mu_L(L \cup R) > 0$, and thus, 
    $\beta_{W_G}(L,R)$ lies in the set $B$. We will show that $\beta_{W_G}(L,R)
    = \tilde \beta_G(\alpha, \gamma)$, so that we can conclude that 
    $\tilde \beta_G(\alpha, \gamma)$ also belongs to the set $B$. For that, note that
    \begin{align*}
        & \hspace{0.6cm}
        2 \eta(L \times L) + 2 \eta(R \times R) + \eta((L \cup R) \times (L \cup R)^c)
        \\
        & =
        \sum_{i,j \in V} [2 \mu_L(L \cap P_i) \mu_L(L \cap P_j) 
        + 2 \mu_L(R \cap P_i) \mu_L(R \cap P_j)
        \\
        & \quad + 
        \mu_L((L \cup R) \cap P_i) \mu_L((L \cup R)^c \cap P_j)] w_{ij}
        \\
        & =
        \sum_{i,j \in V} \left[2 \frac{\alpha_i}{n} \frac{\alpha_j}{n}
        + 2 \frac{\gamma_i}{n} \frac{\gamma_j}{n} + \left(\frac{\alpha_i + \gamma_i}{n}\right)
        \left(\frac{1 - (\alpha_j + \gamma_j)}{n}\right)\right] w_{ij}
        \\
        & =
        \frac{1}{n^2} \sum_{i,j \in V} [2 \alpha_i \alpha_j + 2 \gamma_i \gamma_j 
        + (\alpha_i + \gamma_i)(1 - (\alpha_j + \gamma_j))] w_{ij},
    \end{align*}
    and that 
    \begin{align*}
        2 \eta((L \cup R) \times I) 
        & = 
        2 \sum_{i,j \in V} \mu_L((L \cup R) \cap P_i) \mu_L(P_j) w_{ij}
        \\
        & =
        \frac{2}{n} \sum_{i,j \in V} \left(\frac{\alpha_i + \gamma_i}{n}\right) w_{ij}
        \\
        & =
        \frac{2}{n^2} \sum_{i,j \in V} (\alpha_i + \gamma_i) w_{ij}.
    \end{align*}
    Combining the above two equations gives us that 
    $\beta_{W_G}(L,R) = \tilde \beta_G(\alpha, \gamma)$, and this proves that 
    $A$ is a subset of $B$. To obtain the other inclusion, start with disjoint
    measurable subsets $L$ and $R$ of $I$ with $\mu_L(L \cup R) > 0$, and set
    $\alpha_i = n \mu_L(L \cap P_i)$ and $\beta_j = n \mu_L(R \cap P_j)$, for every 
    $i,j \in V$. Then the above calculations show that the elements 
    $\alpha = (\alpha_1, \dots, \alpha_n), \gamma = (\gamma_1, \dots, \gamma_n)$ 
    of $I^n$ are such that $0 < \alpha + \gamma \leq 1$ and $\beta_{W_G}(L,R)
    = \tilde \beta_G(\alpha, \gamma)$, implying that $B$ is a subset of $A$.
\end{proof}

From the arguments similar to that in the proof of \cref{lemma:beta-WG-tilde}, 
and the fact that $e_G(A,B) = n^2 \eta(\bigcup_{i \in A, j \in B} P_i \times P_j)$, 
for all subsets $A,B$ of $V$, it follows that
\begin{equation} \label{eq:betaG&betaG-tilde}
    \beta_G 
    = \min_{{\substack{\alpha, \gamma \in \{0,1\}^n \\ 0 < \alpha + \gamma \leq 1}}}
    \tilde \beta_G(\alpha,\gamma).
\end{equation}

The next lemma, which is similar to \cite{Abhishek-Mahan24}*{Lemma 4.1}, 
shows that the infimum in \cref{eq:beta-WG-tilde} is attained.

\begin{lemma} \label{lemma:betaWG-attained}
    Given any connected weighted graph $G = (V,w)$, there exist elements 
    $\alpha, \gamma$ of $I^n$ with $0 < \alpha + \gamma \leq 1$ such that
    $\beta_{W_G} = \tilde \beta_G(\alpha,\gamma)$.
\end{lemma}

\begin{proof}
    If $\beta_{W_G} = \frac{1}{2}$, then we have 
    $\beta_{W_G} = \tilde \beta_G(\alpha,\gamma)$ for 
    $\alpha = \gamma = \left(\frac{1}{2}, \dots, \frac{1}{2}\right) \in I^n$.

    Now assume that $\beta_{W_G} \neq \frac{1}{2}$, that is, $\beta_{W_G} < \frac{1}{2}$,
    by \cref{lemma:beta-leq-half}. Then, using \cref{lemma:beta-WG-tilde}, 
    given any positive integer $k$, there exist elements 
    $\alpha^{(k)} = (\alpha_1^{(k)}, \dots, \alpha_n^{(k)}), 
    \gamma^{(k)} = (\gamma_1^{(k)}, \dots, \gamma_n^{(k)})$ of $I^n$ with 
    $0 < \alpha^{(k)} + \gamma^{(k)} \leq 1$ such that
    \begin{equation} \label{ineq:seqI2n-betaWG}
        \beta_{W_G} \leq \tilde \beta_G(\alpha^{(k)},\gamma^{(k)}) 
        < \beta_{W_G} + \frac{1}{k}.
    \end{equation}
    Since $I^n$ is compact, the sequences $(\alpha^{(k)})$ and $(\gamma^{(k)})$ 
    have convergent subsequences in $I^n$, which we again denote by 
    $(\alpha^{(k)})$ and $(\gamma^{(k)})$, respectively, abusing the notation. 
    Suppose they converge to $\alpha$ and $\gamma$, respectively, in $I^n$. 
    Then for all $i,j \in V$, the sequences $(\alpha_i^{(k)})$ and $(\gamma_j^{(k)})$ 
    converge to $\alpha_i$ and $\gamma_j$, respectively, in $I$. 
    Consequently, if $(1 \geq)\, \alpha + \gamma > 0$, then the sequence
    $\left(\tilde \beta_G(\alpha^{(k)},\gamma^{(k)})\right)$ converges to 
    $\tilde \beta_G(\alpha,\gamma)$ in $\mathbb{R}$. Then, using \cref{ineq:seqI2n-betaWG},
    it follows that $\beta_{W_G} = \tilde \beta_G(\alpha,\gamma)$. 
    We now show that the case $\alpha + \gamma = 0$ is not possible. 
    
    If $\alpha + \gamma$ is $0$, then there is a positive integer $N$ such that 
    for all $k \geq N$ and $j \in V$, we have $\alpha_j^{(k)} + \gamma_j^{(k)} < \delta$, 
    where $\delta = \frac{1}{2} - \beta_{W_G}$. Hence, for all $k \geq N$, we get
    \begin{align*}
        & \hspace{0.6cm}
        \sum_{i,j \in V} [2 \alpha_i^{(k)} \alpha_j^{(k)} + 2 \gamma_i^{(k)} \gamma_j^{(k)} 
        + (\alpha_i^{(k)} + \gamma_i^{(k)})(1 - (\alpha_j^{(k)} + \gamma_j^{(k)}))] w_{ij}
        \\
        & \geq
        \sum_{i,j \in V} (\alpha_i^{(k)} 
        + \gamma_i^{(k)})(1 - (\alpha_j^{(k)} + \gamma_j^{(k)})) w_{ij}
        \\
        & \geq
        (1 - \delta) \sum_{i,j \in V} (\alpha_i^{(k)} + \gamma_i^{(k)}) w_{ij},
    \end{align*}
    which implies that 
    $\tilde \beta_G(\alpha^{(k)}, \gamma^{(k)}) \geq \frac{1 -\delta}{2}$, for all $k \geq N$.
    Combining this with \cref{ineq:seqI2n-betaWG} gives
    \begin{equation*}
        \frac{1 -\delta}{2} = \frac{1}{4} + \frac{\beta_{W_G}}{2} 
        < \beta_{W_G} + \frac{1}{k},
    \end{equation*}
    equivalently, $k < \frac{4}{1 - 2 \beta_{W_G}}$ for all $k \geq N$, which is impossible.
\end{proof}

In the following lemma, we compare the bipartiteness ratios of graphs and 
the associated graphons, using certain ``suitable'' random variables. 
The analogous result for the Cheeger constants is discussed in \cite{Abhishek-Mahan24}*{Lemma 4.4}.

\begin{lemma} \label{lemma:compare-betaG&WG}
    For every loopless, connected weighted graph $G = (V,w)$, the following inequality holds.
    \begin{equation*}
        \frac{1}{4} \beta_{G} \leq \beta_{W_G} \leq \beta_G.
    \end{equation*}
\end{lemma}

\begin{proof}
    It is clear from \cref{lemma:beta-WG-tilde} and \cref{eq:betaG&betaG-tilde} 
    that $\beta_{W_G} \leq \beta_G$. We proceed to prove the other inequality.
    Let $\alpha = (\alpha_1, \dots, \alpha_n)$ and $\gamma = (\gamma_1, \dots, \gamma_n)$ 
    be elements of $I^n$ with $0 < \alpha + \gamma \leq 1$ satisfying 
    $\beta_{W_G} = \tilde \beta_G(\alpha, \gamma)$. 
    The existence of such elements is guaranteed by \cref{lemma:betaWG-attained}.

    Let $L_1, \dots, L_n$ and $R_1, \dots, R_n$ be independent random variables
    on some probability space $(\Omega, \mathcal{A}, P)$ such that 
    $P(L_i^{-1}(1)) = \alpha_i$, $P(L_i^{-1}(0)) = 1 - \alpha_i$, 
    $P(R_i^{-1}(1)) = \gamma_i$, and $P(R_i^{-1}(0)) = 1 - \gamma_i$, for all $1 \leq i \leq n$.
    Define random variables $X$ and $Y$ as follows.
    \begin{equation*}
        X = \sum_{i,j \in V} [2 L_i L_j + 2 R_i R_j + (L_i + R_i)(1 - L_j - R_j)] w_{ij},
    \end{equation*}   
    and
    \begin{equation*} 
        Y = 2 \sum_{i,j \in V} (L_i + R_i) w_{ij}.
    \end{equation*}
    Then, since the graph $G$ is loopless, the expectations of $X$ and $Y$ are
    \begin{equation*}
        E[X] = \sum_{i,j \in V} [2 \alpha_i \alpha_j + 2 \gamma_i \gamma_j 
        + (\alpha_i + \gamma_i)(1 - \alpha_j -\gamma_j)] w_{ij},
    \end{equation*}
    and
    \begin{equation*}
        E[Y] = 2 \sum_{i,j \in V} (\alpha_i + \gamma_i) w_{ij}. 
    \end{equation*}
    We will now show that the inequality $X(\omega) \geq \frac{1}{4} \beta_G Y(\omega)$
    holds for all $\omega \in \Omega$. Let $\omega \in \Omega$ be arbitrary.
    Consider the set $S = \{i \in V : L_i(\omega) = R_i(\omega) = 1\}$. 
    If $S$ is the empty set, then we get $X(\omega) \geq \beta_G Y(\omega)$
    from \cref{eq:betaG&betaG-tilde}, and we are done. Suppose that the set $S$ is nonempty.
    Denote by $x = (x_1, \dots, x_n)$ and $y = (y_1, \dots, y_n)$, the elements of $I^n$
    defined by 
    \begin{equation*}
        x_i = L_i(\omega) \quad \text{and} \quad y_i = 
        \begin{cases}
            0 & \text{if } i \in S,
            \\
            R_i(\omega) & \text{if } i \notin S,
        \end{cases} 
    \end{equation*}
    for all $1 \leq i \leq n$. Note that $x$ and $y$ are elements of $\{0,1\}^n$, 
    and they satisfy $0 < x + y \leq 1$. So, thanks to \cref{eq:betaG&betaG-tilde}, 
    it suffices to prove that $X(\omega) \geq \frac{1}{4} \tilde \beta_G(x,y) Y(\omega)$.
    Observe that
    \begin{align*}
        X(\omega) 
        & = 
        \sum_{i,j \notin S} 
        (2 L_i L_j + 2 R_i R_j + (L_i + R_i)(1 - L_j - R_j))(\omega) w_{ij}
        \\
        & \quad +
        \sum_{i \notin S, j \in S} 
        (2 L_i L_j + 2 R_i R_j + (L_i + R_i)(1 - L_j - R_j))(\omega) w_{ij}
        \\
        & \quad +
        \sum_{i \in S, j \notin S} 
        (2 L_i L_j + 2 R_i R_j + (L_i + R_i)(1 - L_j - R_j))(\omega) w_{ij}
        \\
        & \quad +
        \sum_{i \in S, j \in S} 
        (2 L_i L_j + 2 R_i R_j + (L_i + R_i)(1 - L_j - R_j))(\omega) w_{ij}
        \\
        & =
        \sum_{i,j \notin S} 
        (2 L_i L_j + 2 R_i R_j + (L_i + R_i)(1 - L_j - R_j))(\omega) w_{ij}
        \\
        & \quad +
        \sum_{i \notin S, j \in S} (L_i + R_i)(\omega) w_{ij}
        + \sum_{i \in S, j \notin S} 2 w_{ij} + \sum_{i \in S, j \in S} 2 w_{ij}
        \\
        & \geq
        \sum_{i,j \notin S} [2 x_i x_j + 2 y_i y_j + (x_i + y_i)(1 - x_j - y_j)] w_{ij}
        \\
        & \quad +
        \frac{1}{2} \sum_{i \notin S, j \in S} 2x_i w_{ij} 
        + \sum_{i \in S, j \notin S} (2x_j + 1 - x_j - y_j) w_{ij}
        + \sum_{i \in S, j \in S} 2 w_{ij}
        \\
        & \geq
        \frac{1}{2} \sum_{i,j \in V} 
        [2 x_i x_j + 2 y_i y_j + (x_i + y_i)(1 - x_j - y_j)] w_{ij},
    \end{align*}
    and that
    \begin{align*}
        Y(\omega) 
        & = 
        2 \sum_{i \notin S, j \in V} (L_i + R_i)(\omega) w_{ij}
        + 2 \sum_{i \in S, j \in V} (L_i + R_i)(\omega) w_{ij}
        \\
        & =
        2 \sum_{i \notin S, j \in V} (x_i + y_i) w_{ij} 
        + 2 \sum_{i \in S, j \in V} 2 (x_i + y_i) w_{ij}
        \\
        & \leq 
        2 \cdot 2 \sum_{i,j \in V} (x_i + y_i) w_{ij}.
    \end{align*}
    Thus, we arrive at the inequality 
    \begin{equation*}
        X(\omega) \geq \frac{1}{4} \tilde \beta_G(x,y) Y(\omega) 
        \geq \frac{1}{4} \beta_G Y(\omega). 
    \end{equation*}
    As this is true for all $\omega \in \Omega$, it implies that 
    $E[X] \geq \frac{1}{4} \beta_G E[Y]$, that is, 
    \begin{equation*}
        \frac{E[X]}{E[Y]} = \beta_{W_G} \geq \frac{1}{4} \beta_G,
    \end{equation*}
    using the fact that $E[Y]$ is positive.
\end{proof}

\begin{remark}
    Given any loopless, connected weighted graph $G$, combining \cref{thm:main}, 
    \cref{lemma:compare-lambdaG&WG} and \cref{lemma:compare-betaG&WG} yields the inequality
    \begin{equation} \label{ineq:weak-dualCB}
        \frac{\beta_G^2}{32} \leq 2 - \lambda_G^{\max} \leq 2 \beta_G.
    \end{equation}
    Define the linear operator $\map{T}{\ell^2(V)}{\ell^2(V)}$ by 
    $(Tf)(i) = \sum_{j \in V} w_{ij} f_j$. Then the Laplacian $\Delta_G$ of $G$
    is same as the operator $L$ defined in \cref{ch:dualCBGraphs}. Further, 
    let $\mathcal{S}_1$ denote the set of all nonzero functions from $V$ to $\{-1,0,1\}$,
    and $\mathcal{S}_2$ denote the set 
    \begin{equation*}
        \{(A,B) \subseteq V \times V : A \cap B = \emptyset, A \cup B \neq \emptyset\}.
    \end{equation*}
    Then the function which assigns every function $f$ in $\mathcal{S}_1$ to 
    the ordered pair $(A,B)$ in $\mathcal{S}_2$, where $A = f^{-1}(-1)$ and $B = f^{-1}(1)$, 
    is a bijection satisfying the equation
    \begin{align*}
        \frac{\sum_{i,j \in V} w_{ij} (f(i) + f(j))^2}{4 \sum_{i \in V} \vol(i) f(i)^2}
        = 
        \frac{2 e_G(A,A) + 2 e_G(B,B) + e_G(A \cup B, (A \cup B)^c)}{2 e_G(A \cup B,V)}.
    \end{align*}
    Hence, using the definiton of $\beta_G$ and \cref{eq:explicit-betaII}, 
    it follows that $\beta_G = \beta_T$. So, \eqref{ineq:weak-dualCB} is 
    the dual Cheeger--Buser inequality for graphs, up to a multiplicative constant.
\end{remark}

% -----------------------------

% \begin{appendices}
% \renewcommand{\thesection}{\Roman{section}}
% \section{Basic Definitions}

% This is the first section of the appendix.

% \section{Additional Theorems}

% This is the second section of the appendix.

% \end{appendices}
% -----------------------------

\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plain}
\bibliography{../master}

\end{document}

%Mugdha: The commands \clearpage, \phantomsection are for toc to direct us to appropriate pages,
%These, along with \addcont... should come before section name to get directed to 
%the top of the corresponding page